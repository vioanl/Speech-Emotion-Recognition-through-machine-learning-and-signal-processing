{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_clean_list = glob.glob('clean/Users/ioann/radvess-XYMA/*.wav')\n",
    "savee_clean_list = glob.glob('clean/Users/ioann/saveelist/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ravdess_index(file_list):\n",
    "    \n",
    "    emotion_key = {'01': 'neutral', '02': 'neutral', '03': 'happy', '04': 'sad', '05': 'angry', '06': 'fearful', '07': 'disgusted', '08': 'surprised'}\n",
    "    df = { 'emotion': [] }\n",
    "   \n",
    "\n",
    "    for file in file_list:\n",
    "        props = file.split('-')\n",
    "        \n",
    "        df['emotion'].append(emotion_key[props[3]])\n",
    "        \n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "       \n",
    "\n",
    "    file_properties = pd.DataFrame(df)\n",
    "    \n",
    "    return file_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ravdess = build_ravdess_index(ravdess_clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_savee_index(file_list):\n",
    "    \n",
    "    emotion_key = {'n': 'neutral', 'h': 'happy', 'sa': 'sad', 'a': 'angry', 'f': 'fearful', 'd': 'disgusted', 'su': 'surprised'}\n",
    "    df = { 'emotion': [] }\n",
    "   \n",
    "\n",
    "    for file in file_list:\n",
    "        \n",
    "\n",
    "        props = file.split('/')[3][10:]\n",
    "       \n",
    "        df['emotion'].append(emotion_key[props[ :-9]])\n",
    "       \n",
    "\n",
    "       \n",
    "\n",
    "    file_properties = pd.DataFrame(df)\n",
    "    \n",
    "    return file_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_savee=build_savee_index(savee_clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "\n",
    "\n",
    "\n",
    "path = '/Users/ioann/radvess-XYMA/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "      \n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_features = pd.DataFrame(df['feature'].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-700.398878</td>\n",
       "      <td>58.630210</td>\n",
       "      <td>-3.025852</td>\n",
       "      <td>16.040243</td>\n",
       "      <td>4.248529</td>\n",
       "      <td>3.869935</td>\n",
       "      <td>-6.381717</td>\n",
       "      <td>-0.188634</td>\n",
       "      <td>-13.735003</td>\n",
       "      <td>-0.319723</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.411359</td>\n",
       "      <td>-2.769772</td>\n",
       "      <td>-2.042008</td>\n",
       "      <td>-2.522663</td>\n",
       "      <td>-2.507449</td>\n",
       "      <td>-2.250499</td>\n",
       "      <td>-0.381507</td>\n",
       "      <td>-2.481059</td>\n",
       "      <td>-2.791022</td>\n",
       "      <td>-2.244866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-653.169017</td>\n",
       "      <td>58.028073</td>\n",
       "      <td>-12.581209</td>\n",
       "      <td>11.818785</td>\n",
       "      <td>-7.681562</td>\n",
       "      <td>-0.617142</td>\n",
       "      <td>-8.337758</td>\n",
       "      <td>-5.823571</td>\n",
       "      <td>-6.547591</td>\n",
       "      <td>1.458057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788761</td>\n",
       "      <td>2.570493</td>\n",
       "      <td>2.558635</td>\n",
       "      <td>3.708506</td>\n",
       "      <td>2.790020</td>\n",
       "      <td>2.201920</td>\n",
       "      <td>-1.021457</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>-0.277811</td>\n",
       "      <td>0.207586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-617.052327</td>\n",
       "      <td>60.103365</td>\n",
       "      <td>-5.984258</td>\n",
       "      <td>13.886286</td>\n",
       "      <td>1.120427</td>\n",
       "      <td>0.511750</td>\n",
       "      <td>-14.841357</td>\n",
       "      <td>-4.016369</td>\n",
       "      <td>-5.575839</td>\n",
       "      <td>-6.309851</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.133367</td>\n",
       "      <td>-2.019846</td>\n",
       "      <td>-1.024787</td>\n",
       "      <td>0.331097</td>\n",
       "      <td>0.531833</td>\n",
       "      <td>-1.621018</td>\n",
       "      <td>-2.158077</td>\n",
       "      <td>-2.502774</td>\n",
       "      <td>-0.676132</td>\n",
       "      <td>2.089097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-698.721214</td>\n",
       "      <td>47.088487</td>\n",
       "      <td>-11.333611</td>\n",
       "      <td>12.963090</td>\n",
       "      <td>-8.005652</td>\n",
       "      <td>-1.252229</td>\n",
       "      <td>-10.009068</td>\n",
       "      <td>-9.434125</td>\n",
       "      <td>-10.318875</td>\n",
       "      <td>-0.864674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299306</td>\n",
       "      <td>-0.004270</td>\n",
       "      <td>-1.408215</td>\n",
       "      <td>0.958173</td>\n",
       "      <td>2.574661</td>\n",
       "      <td>1.877765</td>\n",
       "      <td>1.608618</td>\n",
       "      <td>0.222401</td>\n",
       "      <td>4.338524</td>\n",
       "      <td>4.266795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-715.922663</td>\n",
       "      <td>71.976798</td>\n",
       "      <td>4.457525</td>\n",
       "      <td>18.602500</td>\n",
       "      <td>8.404048</td>\n",
       "      <td>3.621956</td>\n",
       "      <td>-1.974365</td>\n",
       "      <td>0.022674</td>\n",
       "      <td>-2.678083</td>\n",
       "      <td>4.232804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505697</td>\n",
       "      <td>0.424780</td>\n",
       "      <td>0.972097</td>\n",
       "      <td>0.210004</td>\n",
       "      <td>0.667769</td>\n",
       "      <td>1.918957</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-1.768463</td>\n",
       "      <td>-1.512281</td>\n",
       "      <td>-0.253280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-739.440873</td>\n",
       "      <td>55.755429</td>\n",
       "      <td>-23.246592</td>\n",
       "      <td>13.280397</td>\n",
       "      <td>-3.838021</td>\n",
       "      <td>-5.957225</td>\n",
       "      <td>-12.653000</td>\n",
       "      <td>-6.214706</td>\n",
       "      <td>-10.689208</td>\n",
       "      <td>3.398455</td>\n",
       "      <td>...</td>\n",
       "      <td>4.583356</td>\n",
       "      <td>5.935799</td>\n",
       "      <td>4.206937</td>\n",
       "      <td>5.363243</td>\n",
       "      <td>4.332376</td>\n",
       "      <td>3.667531</td>\n",
       "      <td>1.550318</td>\n",
       "      <td>-0.558750</td>\n",
       "      <td>1.737009</td>\n",
       "      <td>3.253290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-653.982540</td>\n",
       "      <td>77.230163</td>\n",
       "      <td>1.074312</td>\n",
       "      <td>23.132712</td>\n",
       "      <td>5.029030</td>\n",
       "      <td>2.561855</td>\n",
       "      <td>-10.727559</td>\n",
       "      <td>-3.295055</td>\n",
       "      <td>-1.869739</td>\n",
       "      <td>-0.463441</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.285201</td>\n",
       "      <td>-0.391933</td>\n",
       "      <td>-1.683185</td>\n",
       "      <td>-1.358684</td>\n",
       "      <td>-2.281433</td>\n",
       "      <td>-3.152291</td>\n",
       "      <td>-5.220386</td>\n",
       "      <td>-3.469335</td>\n",
       "      <td>-1.696725</td>\n",
       "      <td>-0.342364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-692.393582</td>\n",
       "      <td>68.048347</td>\n",
       "      <td>3.857427</td>\n",
       "      <td>26.590947</td>\n",
       "      <td>2.598323</td>\n",
       "      <td>6.667670</td>\n",
       "      <td>1.640871</td>\n",
       "      <td>-4.939541</td>\n",
       "      <td>-1.004390</td>\n",
       "      <td>1.792656</td>\n",
       "      <td>...</td>\n",
       "      <td>1.513685</td>\n",
       "      <td>1.524577</td>\n",
       "      <td>2.985039</td>\n",
       "      <td>5.605745</td>\n",
       "      <td>5.896914</td>\n",
       "      <td>5.154771</td>\n",
       "      <td>4.143033</td>\n",
       "      <td>5.628289</td>\n",
       "      <td>4.912781</td>\n",
       "      <td>4.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-741.353967</td>\n",
       "      <td>96.758128</td>\n",
       "      <td>16.994427</td>\n",
       "      <td>37.401855</td>\n",
       "      <td>17.461665</td>\n",
       "      <td>14.264806</td>\n",
       "      <td>1.034319</td>\n",
       "      <td>9.790047</td>\n",
       "      <td>3.433929</td>\n",
       "      <td>9.897499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.575394</td>\n",
       "      <td>2.135275</td>\n",
       "      <td>1.080048</td>\n",
       "      <td>0.077569</td>\n",
       "      <td>1.954016</td>\n",
       "      <td>0.924915</td>\n",
       "      <td>-0.342838</td>\n",
       "      <td>-1.029033</td>\n",
       "      <td>-1.741040</td>\n",
       "      <td>-0.339235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-574.430922</td>\n",
       "      <td>56.297725</td>\n",
       "      <td>-3.995249</td>\n",
       "      <td>9.307219</td>\n",
       "      <td>0.660650</td>\n",
       "      <td>0.722855</td>\n",
       "      <td>-10.766332</td>\n",
       "      <td>-3.891049</td>\n",
       "      <td>-7.494775</td>\n",
       "      <td>1.972640</td>\n",
       "      <td>...</td>\n",
       "      <td>3.434052</td>\n",
       "      <td>3.164500</td>\n",
       "      <td>5.605457</td>\n",
       "      <td>3.372160</td>\n",
       "      <td>5.965045</td>\n",
       "      <td>2.542946</td>\n",
       "      <td>2.380176</td>\n",
       "      <td>1.891932</td>\n",
       "      <td>1.244445</td>\n",
       "      <td>3.589855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-734.559104</td>\n",
       "      <td>66.724170</td>\n",
       "      <td>19.973302</td>\n",
       "      <td>24.432135</td>\n",
       "      <td>8.267443</td>\n",
       "      <td>10.193674</td>\n",
       "      <td>3.384751</td>\n",
       "      <td>5.832733</td>\n",
       "      <td>-2.836397</td>\n",
       "      <td>4.247270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924266</td>\n",
       "      <td>0.830266</td>\n",
       "      <td>0.951789</td>\n",
       "      <td>0.409593</td>\n",
       "      <td>-0.970139</td>\n",
       "      <td>-0.998077</td>\n",
       "      <td>-0.801911</td>\n",
       "      <td>-1.194514</td>\n",
       "      <td>-1.572553</td>\n",
       "      <td>-0.679407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-686.793732</td>\n",
       "      <td>63.759015</td>\n",
       "      <td>-5.019265</td>\n",
       "      <td>14.655203</td>\n",
       "      <td>-2.392172</td>\n",
       "      <td>-0.812546</td>\n",
       "      <td>-5.175949</td>\n",
       "      <td>-6.032188</td>\n",
       "      <td>-13.602619</td>\n",
       "      <td>-1.257705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843919</td>\n",
       "      <td>2.624703</td>\n",
       "      <td>3.302091</td>\n",
       "      <td>4.932687</td>\n",
       "      <td>5.131727</td>\n",
       "      <td>3.666039</td>\n",
       "      <td>4.504458</td>\n",
       "      <td>1.554605</td>\n",
       "      <td>0.302437</td>\n",
       "      <td>-2.583656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-737.222784</td>\n",
       "      <td>68.615129</td>\n",
       "      <td>-0.249202</td>\n",
       "      <td>14.366146</td>\n",
       "      <td>6.971780</td>\n",
       "      <td>4.769932</td>\n",
       "      <td>-3.964833</td>\n",
       "      <td>0.740859</td>\n",
       "      <td>-6.925254</td>\n",
       "      <td>3.283133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715441</td>\n",
       "      <td>-2.216875</td>\n",
       "      <td>0.222590</td>\n",
       "      <td>0.218809</td>\n",
       "      <td>0.430849</td>\n",
       "      <td>0.554459</td>\n",
       "      <td>-0.558221</td>\n",
       "      <td>-1.556306</td>\n",
       "      <td>-1.882601</td>\n",
       "      <td>-0.562863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-765.338586</td>\n",
       "      <td>47.169830</td>\n",
       "      <td>1.476838</td>\n",
       "      <td>15.077700</td>\n",
       "      <td>-10.482029</td>\n",
       "      <td>-1.461810</td>\n",
       "      <td>-8.149069</td>\n",
       "      <td>-4.225049</td>\n",
       "      <td>-7.926242</td>\n",
       "      <td>0.051971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.974465</td>\n",
       "      <td>-1.076629</td>\n",
       "      <td>-0.515905</td>\n",
       "      <td>0.504372</td>\n",
       "      <td>1.259402</td>\n",
       "      <td>2.089944</td>\n",
       "      <td>3.679059</td>\n",
       "      <td>2.809832</td>\n",
       "      <td>1.927483</td>\n",
       "      <td>1.177062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-638.686598</td>\n",
       "      <td>72.511378</td>\n",
       "      <td>-5.997497</td>\n",
       "      <td>15.943515</td>\n",
       "      <td>7.175803</td>\n",
       "      <td>1.197456</td>\n",
       "      <td>-1.932479</td>\n",
       "      <td>1.161894</td>\n",
       "      <td>-6.393956</td>\n",
       "      <td>1.198193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569476</td>\n",
       "      <td>-0.153124</td>\n",
       "      <td>0.349048</td>\n",
       "      <td>-0.381267</td>\n",
       "      <td>-0.498064</td>\n",
       "      <td>0.580211</td>\n",
       "      <td>-0.288043</td>\n",
       "      <td>1.125915</td>\n",
       "      <td>0.291572</td>\n",
       "      <td>0.333098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-663.830633</td>\n",
       "      <td>61.380003</td>\n",
       "      <td>2.275193</td>\n",
       "      <td>12.700737</td>\n",
       "      <td>-4.215593</td>\n",
       "      <td>-4.703434</td>\n",
       "      <td>-11.695962</td>\n",
       "      <td>-3.269569</td>\n",
       "      <td>-8.981364</td>\n",
       "      <td>-0.019918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993280</td>\n",
       "      <td>-0.269033</td>\n",
       "      <td>0.044593</td>\n",
       "      <td>-0.209263</td>\n",
       "      <td>-0.497776</td>\n",
       "      <td>0.500730</td>\n",
       "      <td>1.943027</td>\n",
       "      <td>2.300725</td>\n",
       "      <td>2.240043</td>\n",
       "      <td>1.939023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-646.566034</td>\n",
       "      <td>78.240240</td>\n",
       "      <td>5.812849</td>\n",
       "      <td>23.272988</td>\n",
       "      <td>13.461742</td>\n",
       "      <td>11.165088</td>\n",
       "      <td>6.540210</td>\n",
       "      <td>-2.067871</td>\n",
       "      <td>-7.179390</td>\n",
       "      <td>9.426169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.517587</td>\n",
       "      <td>-0.507614</td>\n",
       "      <td>1.068280</td>\n",
       "      <td>-0.431788</td>\n",
       "      <td>-0.993862</td>\n",
       "      <td>-1.745499</td>\n",
       "      <td>-0.998088</td>\n",
       "      <td>-1.758631</td>\n",
       "      <td>-1.363380</td>\n",
       "      <td>-2.398707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-754.981614</td>\n",
       "      <td>59.807533</td>\n",
       "      <td>0.850680</td>\n",
       "      <td>15.180320</td>\n",
       "      <td>-4.610334</td>\n",
       "      <td>-0.571079</td>\n",
       "      <td>-6.452572</td>\n",
       "      <td>-9.619606</td>\n",
       "      <td>-11.002419</td>\n",
       "      <td>2.098613</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.768783</td>\n",
       "      <td>-1.801866</td>\n",
       "      <td>-3.324028</td>\n",
       "      <td>-1.853533</td>\n",
       "      <td>-1.035654</td>\n",
       "      <td>0.965440</td>\n",
       "      <td>4.182645</td>\n",
       "      <td>7.387866</td>\n",
       "      <td>10.678265</td>\n",
       "      <td>10.405077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-723.215910</td>\n",
       "      <td>80.465438</td>\n",
       "      <td>8.933278</td>\n",
       "      <td>21.701921</td>\n",
       "      <td>4.516072</td>\n",
       "      <td>10.858137</td>\n",
       "      <td>1.185063</td>\n",
       "      <td>0.157663</td>\n",
       "      <td>-8.158168</td>\n",
       "      <td>3.154788</td>\n",
       "      <td>...</td>\n",
       "      <td>2.799036</td>\n",
       "      <td>-0.825371</td>\n",
       "      <td>-0.517987</td>\n",
       "      <td>1.425363</td>\n",
       "      <td>-0.862558</td>\n",
       "      <td>1.228814</td>\n",
       "      <td>-0.960936</td>\n",
       "      <td>-2.644739</td>\n",
       "      <td>-2.159275</td>\n",
       "      <td>-0.943426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-719.650165</td>\n",
       "      <td>40.265067</td>\n",
       "      <td>-10.677992</td>\n",
       "      <td>12.497934</td>\n",
       "      <td>-10.565180</td>\n",
       "      <td>-2.727621</td>\n",
       "      <td>-11.100496</td>\n",
       "      <td>-7.746538</td>\n",
       "      <td>-8.434710</td>\n",
       "      <td>-1.384802</td>\n",
       "      <td>...</td>\n",
       "      <td>2.023025</td>\n",
       "      <td>2.927179</td>\n",
       "      <td>2.053068</td>\n",
       "      <td>2.236037</td>\n",
       "      <td>2.297235</td>\n",
       "      <td>3.286657</td>\n",
       "      <td>2.094471</td>\n",
       "      <td>1.377533</td>\n",
       "      <td>-1.344257</td>\n",
       "      <td>-1.004930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-596.031184</td>\n",
       "      <td>69.309364</td>\n",
       "      <td>0.296093</td>\n",
       "      <td>16.458553</td>\n",
       "      <td>5.692145</td>\n",
       "      <td>-2.806871</td>\n",
       "      <td>0.449962</td>\n",
       "      <td>-1.387645</td>\n",
       "      <td>-11.512282</td>\n",
       "      <td>2.076061</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.046698</td>\n",
       "      <td>-1.948009</td>\n",
       "      <td>-2.145055</td>\n",
       "      <td>-1.342209</td>\n",
       "      <td>-1.250720</td>\n",
       "      <td>-0.920083</td>\n",
       "      <td>-2.509349</td>\n",
       "      <td>-2.264665</td>\n",
       "      <td>-1.518574</td>\n",
       "      <td>-2.511123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-705.119350</td>\n",
       "      <td>59.930869</td>\n",
       "      <td>-3.234201</td>\n",
       "      <td>10.848731</td>\n",
       "      <td>-0.405465</td>\n",
       "      <td>2.574034</td>\n",
       "      <td>-9.002455</td>\n",
       "      <td>-1.877024</td>\n",
       "      <td>-15.148181</td>\n",
       "      <td>1.105963</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377368</td>\n",
       "      <td>2.297907</td>\n",
       "      <td>-0.597607</td>\n",
       "      <td>1.191020</td>\n",
       "      <td>1.685758</td>\n",
       "      <td>4.253395</td>\n",
       "      <td>5.946149</td>\n",
       "      <td>6.300133</td>\n",
       "      <td>2.370302</td>\n",
       "      <td>4.118457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-647.665136</td>\n",
       "      <td>66.325564</td>\n",
       "      <td>-8.796494</td>\n",
       "      <td>13.239719</td>\n",
       "      <td>7.799501</td>\n",
       "      <td>-1.948146</td>\n",
       "      <td>-7.762551</td>\n",
       "      <td>-0.681861</td>\n",
       "      <td>-12.760769</td>\n",
       "      <td>-2.712420</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.227505</td>\n",
       "      <td>-1.646200</td>\n",
       "      <td>-2.824971</td>\n",
       "      <td>-1.618364</td>\n",
       "      <td>-2.738211</td>\n",
       "      <td>-0.041697</td>\n",
       "      <td>-0.445510</td>\n",
       "      <td>-0.147109</td>\n",
       "      <td>-0.443362</td>\n",
       "      <td>0.124879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-689.293025</td>\n",
       "      <td>46.676768</td>\n",
       "      <td>-10.846545</td>\n",
       "      <td>4.358803</td>\n",
       "      <td>-11.270014</td>\n",
       "      <td>-4.636991</td>\n",
       "      <td>-14.147990</td>\n",
       "      <td>-9.910705</td>\n",
       "      <td>-11.034352</td>\n",
       "      <td>-4.430380</td>\n",
       "      <td>...</td>\n",
       "      <td>4.569240</td>\n",
       "      <td>6.352039</td>\n",
       "      <td>7.300994</td>\n",
       "      <td>5.808718</td>\n",
       "      <td>4.554309</td>\n",
       "      <td>1.843078</td>\n",
       "      <td>0.787600</td>\n",
       "      <td>0.603052</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>-0.394463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-695.557828</td>\n",
       "      <td>59.240152</td>\n",
       "      <td>-5.372777</td>\n",
       "      <td>19.776367</td>\n",
       "      <td>5.200386</td>\n",
       "      <td>3.324629</td>\n",
       "      <td>-4.668407</td>\n",
       "      <td>-2.193975</td>\n",
       "      <td>-14.508076</td>\n",
       "      <td>1.307903</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.055913</td>\n",
       "      <td>-3.102514</td>\n",
       "      <td>-1.697880</td>\n",
       "      <td>-2.922661</td>\n",
       "      <td>-2.544465</td>\n",
       "      <td>-1.289832</td>\n",
       "      <td>-0.797254</td>\n",
       "      <td>-3.586074</td>\n",
       "      <td>-2.706395</td>\n",
       "      <td>-2.812933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-648.336287</td>\n",
       "      <td>52.471800</td>\n",
       "      <td>-14.081988</td>\n",
       "      <td>14.690887</td>\n",
       "      <td>-8.900879</td>\n",
       "      <td>-0.140372</td>\n",
       "      <td>-8.049234</td>\n",
       "      <td>-4.748140</td>\n",
       "      <td>-5.140948</td>\n",
       "      <td>0.179288</td>\n",
       "      <td>...</td>\n",
       "      <td>1.138597</td>\n",
       "      <td>3.222392</td>\n",
       "      <td>3.672537</td>\n",
       "      <td>3.186997</td>\n",
       "      <td>2.064213</td>\n",
       "      <td>2.999678</td>\n",
       "      <td>-0.035473</td>\n",
       "      <td>1.505137</td>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-0.642433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-621.232796</td>\n",
       "      <td>71.126636</td>\n",
       "      <td>1.423355</td>\n",
       "      <td>20.778776</td>\n",
       "      <td>7.105571</td>\n",
       "      <td>5.159855</td>\n",
       "      <td>-11.032454</td>\n",
       "      <td>-0.481391</td>\n",
       "      <td>-2.107458</td>\n",
       "      <td>-1.794385</td>\n",
       "      <td>...</td>\n",
       "      <td>1.418735</td>\n",
       "      <td>0.628589</td>\n",
       "      <td>0.983358</td>\n",
       "      <td>1.843995</td>\n",
       "      <td>2.002833</td>\n",
       "      <td>1.518636</td>\n",
       "      <td>1.326444</td>\n",
       "      <td>1.038551</td>\n",
       "      <td>1.285298</td>\n",
       "      <td>1.143829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-701.859704</td>\n",
       "      <td>48.038854</td>\n",
       "      <td>-11.436011</td>\n",
       "      <td>10.035827</td>\n",
       "      <td>-11.794776</td>\n",
       "      <td>-4.020213</td>\n",
       "      <td>-11.892638</td>\n",
       "      <td>-11.610067</td>\n",
       "      <td>-13.811998</td>\n",
       "      <td>-2.650048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344915</td>\n",
       "      <td>2.232102</td>\n",
       "      <td>2.180859</td>\n",
       "      <td>-0.205000</td>\n",
       "      <td>-1.200028</td>\n",
       "      <td>-1.141648</td>\n",
       "      <td>1.193933</td>\n",
       "      <td>-0.171482</td>\n",
       "      <td>2.330363</td>\n",
       "      <td>4.330539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-733.987504</td>\n",
       "      <td>75.365506</td>\n",
       "      <td>0.546373</td>\n",
       "      <td>23.461105</td>\n",
       "      <td>8.043523</td>\n",
       "      <td>6.832906</td>\n",
       "      <td>-0.417122</td>\n",
       "      <td>-0.153737</td>\n",
       "      <td>-1.305530</td>\n",
       "      <td>6.911249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693509</td>\n",
       "      <td>1.221129</td>\n",
       "      <td>0.807871</td>\n",
       "      <td>0.241845</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>2.517624</td>\n",
       "      <td>1.372746</td>\n",
       "      <td>-1.389155</td>\n",
       "      <td>-2.925384</td>\n",
       "      <td>-0.651365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-700.601184</td>\n",
       "      <td>57.836145</td>\n",
       "      <td>-23.408566</td>\n",
       "      <td>18.187676</td>\n",
       "      <td>-4.776364</td>\n",
       "      <td>-8.323164</td>\n",
       "      <td>-13.772667</td>\n",
       "      <td>-5.920182</td>\n",
       "      <td>-7.138942</td>\n",
       "      <td>4.960144</td>\n",
       "      <td>...</td>\n",
       "      <td>4.195435</td>\n",
       "      <td>5.989556</td>\n",
       "      <td>5.457668</td>\n",
       "      <td>8.291785</td>\n",
       "      <td>8.284182</td>\n",
       "      <td>7.364994</td>\n",
       "      <td>3.067757</td>\n",
       "      <td>-0.109833</td>\n",
       "      <td>3.187426</td>\n",
       "      <td>6.803388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>-429.203846</td>\n",
       "      <td>62.085675</td>\n",
       "      <td>-18.523041</td>\n",
       "      <td>14.571932</td>\n",
       "      <td>-1.515850</td>\n",
       "      <td>6.183253</td>\n",
       "      <td>-8.046456</td>\n",
       "      <td>-1.481375</td>\n",
       "      <td>-17.532570</td>\n",
       "      <td>2.494749</td>\n",
       "      <td>...</td>\n",
       "      <td>2.764888</td>\n",
       "      <td>2.381522</td>\n",
       "      <td>4.564110</td>\n",
       "      <td>4.151742</td>\n",
       "      <td>4.128974</td>\n",
       "      <td>3.486980</td>\n",
       "      <td>6.217521</td>\n",
       "      <td>6.614717</td>\n",
       "      <td>8.534979</td>\n",
       "      <td>8.920184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>-476.160670</td>\n",
       "      <td>57.733493</td>\n",
       "      <td>-27.283853</td>\n",
       "      <td>12.225064</td>\n",
       "      <td>-8.159843</td>\n",
       "      <td>-3.457179</td>\n",
       "      <td>-18.587796</td>\n",
       "      <td>-10.986997</td>\n",
       "      <td>-20.024006</td>\n",
       "      <td>-2.851773</td>\n",
       "      <td>...</td>\n",
       "      <td>4.372591</td>\n",
       "      <td>2.839557</td>\n",
       "      <td>5.506363</td>\n",
       "      <td>7.651137</td>\n",
       "      <td>6.603731</td>\n",
       "      <td>5.822065</td>\n",
       "      <td>5.694686</td>\n",
       "      <td>2.824640</td>\n",
       "      <td>5.872091</td>\n",
       "      <td>8.519549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>-389.781932</td>\n",
       "      <td>29.680010</td>\n",
       "      <td>-34.155934</td>\n",
       "      <td>-7.672322</td>\n",
       "      <td>-25.545736</td>\n",
       "      <td>-18.100876</td>\n",
       "      <td>-14.895015</td>\n",
       "      <td>-9.886096</td>\n",
       "      <td>-15.466568</td>\n",
       "      <td>0.828662</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.102575</td>\n",
       "      <td>-0.868613</td>\n",
       "      <td>-1.742941</td>\n",
       "      <td>-2.597116</td>\n",
       "      <td>-0.986093</td>\n",
       "      <td>0.337788</td>\n",
       "      <td>0.774260</td>\n",
       "      <td>0.300542</td>\n",
       "      <td>5.688090</td>\n",
       "      <td>3.684110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>-477.425029</td>\n",
       "      <td>57.416271</td>\n",
       "      <td>-22.884046</td>\n",
       "      <td>11.699640</td>\n",
       "      <td>-1.564553</td>\n",
       "      <td>-5.759070</td>\n",
       "      <td>-18.075790</td>\n",
       "      <td>3.671213</td>\n",
       "      <td>-28.971663</td>\n",
       "      <td>-2.015684</td>\n",
       "      <td>...</td>\n",
       "      <td>1.385775</td>\n",
       "      <td>-1.550216</td>\n",
       "      <td>1.814287</td>\n",
       "      <td>-0.701365</td>\n",
       "      <td>-4.168486</td>\n",
       "      <td>1.610681</td>\n",
       "      <td>2.516717</td>\n",
       "      <td>6.146086</td>\n",
       "      <td>8.880870</td>\n",
       "      <td>12.728390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>-463.881637</td>\n",
       "      <td>25.228983</td>\n",
       "      <td>-19.256518</td>\n",
       "      <td>-0.568383</td>\n",
       "      <td>-15.849786</td>\n",
       "      <td>-8.165686</td>\n",
       "      <td>-18.913574</td>\n",
       "      <td>-5.280563</td>\n",
       "      <td>-26.643877</td>\n",
       "      <td>-1.343729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916535</td>\n",
       "      <td>2.345366</td>\n",
       "      <td>-0.727863</td>\n",
       "      <td>-4.293114</td>\n",
       "      <td>-1.566936</td>\n",
       "      <td>0.033171</td>\n",
       "      <td>1.065494</td>\n",
       "      <td>-1.023938</td>\n",
       "      <td>1.883473</td>\n",
       "      <td>6.978850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>-467.551978</td>\n",
       "      <td>58.683119</td>\n",
       "      <td>-22.654533</td>\n",
       "      <td>12.242594</td>\n",
       "      <td>0.267150</td>\n",
       "      <td>-7.828310</td>\n",
       "      <td>-16.688125</td>\n",
       "      <td>-4.233726</td>\n",
       "      <td>-16.579934</td>\n",
       "      <td>-1.539022</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.486852</td>\n",
       "      <td>-1.767214</td>\n",
       "      <td>-1.389356</td>\n",
       "      <td>-0.648048</td>\n",
       "      <td>-0.599664</td>\n",
       "      <td>0.738672</td>\n",
       "      <td>3.030808</td>\n",
       "      <td>5.280357</td>\n",
       "      <td>9.240118</td>\n",
       "      <td>11.328468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>-466.656045</td>\n",
       "      <td>34.169956</td>\n",
       "      <td>-13.513101</td>\n",
       "      <td>5.190642</td>\n",
       "      <td>-20.654025</td>\n",
       "      <td>-1.952806</td>\n",
       "      <td>-13.543329</td>\n",
       "      <td>-4.892323</td>\n",
       "      <td>-12.826017</td>\n",
       "      <td>2.937352</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.040943</td>\n",
       "      <td>-2.017374</td>\n",
       "      <td>-1.707237</td>\n",
       "      <td>-1.737038</td>\n",
       "      <td>-0.366111</td>\n",
       "      <td>1.945852</td>\n",
       "      <td>1.262407</td>\n",
       "      <td>2.308048</td>\n",
       "      <td>2.737007</td>\n",
       "      <td>2.305828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>-454.286043</td>\n",
       "      <td>57.456880</td>\n",
       "      <td>-28.010895</td>\n",
       "      <td>9.882527</td>\n",
       "      <td>0.564823</td>\n",
       "      <td>-1.709530</td>\n",
       "      <td>-20.537823</td>\n",
       "      <td>-10.167647</td>\n",
       "      <td>-25.669589</td>\n",
       "      <td>-4.336482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095318</td>\n",
       "      <td>-0.348905</td>\n",
       "      <td>0.913677</td>\n",
       "      <td>-0.077920</td>\n",
       "      <td>-0.197528</td>\n",
       "      <td>2.560753</td>\n",
       "      <td>5.958482</td>\n",
       "      <td>9.160935</td>\n",
       "      <td>9.730867</td>\n",
       "      <td>8.572048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>-469.116970</td>\n",
       "      <td>24.932663</td>\n",
       "      <td>-39.038266</td>\n",
       "      <td>6.984547</td>\n",
       "      <td>-17.489141</td>\n",
       "      <td>-11.372803</td>\n",
       "      <td>-10.220135</td>\n",
       "      <td>-14.558235</td>\n",
       "      <td>-15.385056</td>\n",
       "      <td>2.714475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475706</td>\n",
       "      <td>2.433466</td>\n",
       "      <td>-2.165421</td>\n",
       "      <td>-4.516422</td>\n",
       "      <td>-1.098824</td>\n",
       "      <td>3.480164</td>\n",
       "      <td>-0.090849</td>\n",
       "      <td>-0.770057</td>\n",
       "      <td>6.024706</td>\n",
       "      <td>8.801284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>-363.731149</td>\n",
       "      <td>71.693802</td>\n",
       "      <td>-19.233360</td>\n",
       "      <td>16.502981</td>\n",
       "      <td>-4.358719</td>\n",
       "      <td>10.501375</td>\n",
       "      <td>-15.280219</td>\n",
       "      <td>-8.005958</td>\n",
       "      <td>-11.037171</td>\n",
       "      <td>-5.284053</td>\n",
       "      <td>...</td>\n",
       "      <td>4.400745</td>\n",
       "      <td>6.690452</td>\n",
       "      <td>4.785794</td>\n",
       "      <td>3.915019</td>\n",
       "      <td>3.445365</td>\n",
       "      <td>3.222094</td>\n",
       "      <td>5.648620</td>\n",
       "      <td>9.422134</td>\n",
       "      <td>9.478660</td>\n",
       "      <td>10.289215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>-468.246852</td>\n",
       "      <td>22.862166</td>\n",
       "      <td>-24.003663</td>\n",
       "      <td>5.851704</td>\n",
       "      <td>-16.788944</td>\n",
       "      <td>-7.009547</td>\n",
       "      <td>-9.375116</td>\n",
       "      <td>-15.625277</td>\n",
       "      <td>-12.176636</td>\n",
       "      <td>-0.531862</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.284645</td>\n",
       "      <td>2.262799</td>\n",
       "      <td>-0.304973</td>\n",
       "      <td>-5.778587</td>\n",
       "      <td>-1.170822</td>\n",
       "      <td>0.324805</td>\n",
       "      <td>0.087237</td>\n",
       "      <td>-0.856777</td>\n",
       "      <td>3.920226</td>\n",
       "      <td>6.802940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>-541.059628</td>\n",
       "      <td>52.016956</td>\n",
       "      <td>-8.113971</td>\n",
       "      <td>14.753582</td>\n",
       "      <td>-2.673453</td>\n",
       "      <td>-1.577187</td>\n",
       "      <td>-14.328500</td>\n",
       "      <td>4.092199</td>\n",
       "      <td>-12.480850</td>\n",
       "      <td>0.101191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825417</td>\n",
       "      <td>1.013287</td>\n",
       "      <td>2.891954</td>\n",
       "      <td>2.667731</td>\n",
       "      <td>2.843483</td>\n",
       "      <td>2.167473</td>\n",
       "      <td>4.230566</td>\n",
       "      <td>6.786861</td>\n",
       "      <td>10.595272</td>\n",
       "      <td>13.427793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>-440.992862</td>\n",
       "      <td>32.873942</td>\n",
       "      <td>-44.255752</td>\n",
       "      <td>-2.391997</td>\n",
       "      <td>-19.898011</td>\n",
       "      <td>-19.198309</td>\n",
       "      <td>-17.034746</td>\n",
       "      <td>-8.098862</td>\n",
       "      <td>-17.049275</td>\n",
       "      <td>11.658251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.799569</td>\n",
       "      <td>5.429992</td>\n",
       "      <td>3.519748</td>\n",
       "      <td>3.263165</td>\n",
       "      <td>1.389722</td>\n",
       "      <td>3.486272</td>\n",
       "      <td>1.296781</td>\n",
       "      <td>1.221333</td>\n",
       "      <td>4.486672</td>\n",
       "      <td>5.342615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>-472.640220</td>\n",
       "      <td>53.980659</td>\n",
       "      <td>-27.712259</td>\n",
       "      <td>12.405955</td>\n",
       "      <td>0.535059</td>\n",
       "      <td>-17.107110</td>\n",
       "      <td>-16.421782</td>\n",
       "      <td>-2.927878</td>\n",
       "      <td>-16.100153</td>\n",
       "      <td>-0.669792</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.144668</td>\n",
       "      <td>3.093919</td>\n",
       "      <td>2.414802</td>\n",
       "      <td>4.939812</td>\n",
       "      <td>3.184422</td>\n",
       "      <td>4.305104</td>\n",
       "      <td>2.778225</td>\n",
       "      <td>5.659955</td>\n",
       "      <td>7.980364</td>\n",
       "      <td>7.617889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>-495.574887</td>\n",
       "      <td>27.565998</td>\n",
       "      <td>-30.299083</td>\n",
       "      <td>10.835284</td>\n",
       "      <td>-10.216362</td>\n",
       "      <td>-15.362682</td>\n",
       "      <td>-4.401279</td>\n",
       "      <td>-12.859067</td>\n",
       "      <td>-14.968482</td>\n",
       "      <td>4.055844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753993</td>\n",
       "      <td>4.940733</td>\n",
       "      <td>1.890999</td>\n",
       "      <td>0.468223</td>\n",
       "      <td>2.349840</td>\n",
       "      <td>5.618784</td>\n",
       "      <td>6.002854</td>\n",
       "      <td>2.441198</td>\n",
       "      <td>6.244189</td>\n",
       "      <td>8.364046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>-563.862532</td>\n",
       "      <td>64.251987</td>\n",
       "      <td>-1.875247</td>\n",
       "      <td>23.504682</td>\n",
       "      <td>11.307487</td>\n",
       "      <td>1.261256</td>\n",
       "      <td>-13.603296</td>\n",
       "      <td>3.872112</td>\n",
       "      <td>-13.415906</td>\n",
       "      <td>-1.189780</td>\n",
       "      <td>...</td>\n",
       "      <td>1.547678</td>\n",
       "      <td>2.703273</td>\n",
       "      <td>2.300020</td>\n",
       "      <td>1.774639</td>\n",
       "      <td>3.111295</td>\n",
       "      <td>3.451707</td>\n",
       "      <td>5.322471</td>\n",
       "      <td>4.609474</td>\n",
       "      <td>6.939531</td>\n",
       "      <td>6.491947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-518.318255</td>\n",
       "      <td>32.038844</td>\n",
       "      <td>-13.752025</td>\n",
       "      <td>10.237128</td>\n",
       "      <td>-8.979896</td>\n",
       "      <td>-8.744242</td>\n",
       "      <td>-14.966661</td>\n",
       "      <td>-14.077097</td>\n",
       "      <td>-14.856027</td>\n",
       "      <td>-0.402052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.870467</td>\n",
       "      <td>2.136834</td>\n",
       "      <td>-2.737673</td>\n",
       "      <td>-3.423134</td>\n",
       "      <td>-2.481716</td>\n",
       "      <td>2.306540</td>\n",
       "      <td>-1.143341</td>\n",
       "      <td>1.014231</td>\n",
       "      <td>5.708663</td>\n",
       "      <td>9.272492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-530.325941</td>\n",
       "      <td>58.406973</td>\n",
       "      <td>-4.335286</td>\n",
       "      <td>12.072920</td>\n",
       "      <td>-4.847376</td>\n",
       "      <td>7.940704</td>\n",
       "      <td>-8.287180</td>\n",
       "      <td>-2.737741</td>\n",
       "      <td>-14.531733</td>\n",
       "      <td>-1.755857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182924</td>\n",
       "      <td>0.326268</td>\n",
       "      <td>3.425445</td>\n",
       "      <td>0.514945</td>\n",
       "      <td>-0.334071</td>\n",
       "      <td>-0.168194</td>\n",
       "      <td>1.996263</td>\n",
       "      <td>6.399669</td>\n",
       "      <td>12.176999</td>\n",
       "      <td>11.788868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-428.921656</td>\n",
       "      <td>40.096348</td>\n",
       "      <td>-28.985306</td>\n",
       "      <td>-5.392292</td>\n",
       "      <td>-9.386025</td>\n",
       "      <td>-15.318210</td>\n",
       "      <td>-16.408057</td>\n",
       "      <td>-11.023748</td>\n",
       "      <td>-25.511774</td>\n",
       "      <td>9.051820</td>\n",
       "      <td>...</td>\n",
       "      <td>4.320986</td>\n",
       "      <td>2.382091</td>\n",
       "      <td>-2.431258</td>\n",
       "      <td>-1.943195</td>\n",
       "      <td>1.365404</td>\n",
       "      <td>3.361830</td>\n",
       "      <td>2.462617</td>\n",
       "      <td>1.676635</td>\n",
       "      <td>3.234639</td>\n",
       "      <td>2.727310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-521.366716</td>\n",
       "      <td>54.325254</td>\n",
       "      <td>-18.131659</td>\n",
       "      <td>17.833857</td>\n",
       "      <td>-1.772298</td>\n",
       "      <td>-1.253668</td>\n",
       "      <td>-20.517578</td>\n",
       "      <td>0.791653</td>\n",
       "      <td>-17.416759</td>\n",
       "      <td>1.175464</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.353009</td>\n",
       "      <td>1.468018</td>\n",
       "      <td>-1.404139</td>\n",
       "      <td>1.529970</td>\n",
       "      <td>2.077743</td>\n",
       "      <td>1.659654</td>\n",
       "      <td>2.243726</td>\n",
       "      <td>3.093645</td>\n",
       "      <td>8.210729</td>\n",
       "      <td>10.980291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-550.557945</td>\n",
       "      <td>35.701355</td>\n",
       "      <td>-23.115209</td>\n",
       "      <td>11.718848</td>\n",
       "      <td>-14.158571</td>\n",
       "      <td>-8.739013</td>\n",
       "      <td>-9.842138</td>\n",
       "      <td>-10.146640</td>\n",
       "      <td>-8.128038</td>\n",
       "      <td>0.225512</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.280342</td>\n",
       "      <td>0.386437</td>\n",
       "      <td>-2.974271</td>\n",
       "      <td>0.380171</td>\n",
       "      <td>0.549122</td>\n",
       "      <td>0.822314</td>\n",
       "      <td>0.812847</td>\n",
       "      <td>1.695049</td>\n",
       "      <td>1.728104</td>\n",
       "      <td>0.802639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>-444.586361</td>\n",
       "      <td>47.312312</td>\n",
       "      <td>-23.933954</td>\n",
       "      <td>14.736932</td>\n",
       "      <td>3.365753</td>\n",
       "      <td>-7.494775</td>\n",
       "      <td>-14.385962</td>\n",
       "      <td>0.643082</td>\n",
       "      <td>-22.092245</td>\n",
       "      <td>4.009877</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088949</td>\n",
       "      <td>0.851614</td>\n",
       "      <td>2.509858</td>\n",
       "      <td>1.908783</td>\n",
       "      <td>3.344266</td>\n",
       "      <td>2.556932</td>\n",
       "      <td>5.009627</td>\n",
       "      <td>5.761484</td>\n",
       "      <td>9.606792</td>\n",
       "      <td>10.331644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>-489.893832</td>\n",
       "      <td>36.523780</td>\n",
       "      <td>-28.479019</td>\n",
       "      <td>10.569031</td>\n",
       "      <td>-10.309646</td>\n",
       "      <td>-19.148973</td>\n",
       "      <td>-17.317035</td>\n",
       "      <td>-13.192929</td>\n",
       "      <td>-18.663201</td>\n",
       "      <td>5.794672</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.458641</td>\n",
       "      <td>1.181090</td>\n",
       "      <td>-1.933176</td>\n",
       "      <td>-6.533607</td>\n",
       "      <td>-4.179962</td>\n",
       "      <td>0.479534</td>\n",
       "      <td>2.609350</td>\n",
       "      <td>-2.024513</td>\n",
       "      <td>2.703920</td>\n",
       "      <td>9.721113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>-462.836162</td>\n",
       "      <td>62.963434</td>\n",
       "      <td>-21.001537</td>\n",
       "      <td>10.123778</td>\n",
       "      <td>-0.078926</td>\n",
       "      <td>4.516155</td>\n",
       "      <td>-8.090581</td>\n",
       "      <td>-7.394699</td>\n",
       "      <td>-19.407525</td>\n",
       "      <td>2.447414</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861383</td>\n",
       "      <td>1.817470</td>\n",
       "      <td>6.067732</td>\n",
       "      <td>3.764157</td>\n",
       "      <td>4.073399</td>\n",
       "      <td>2.446536</td>\n",
       "      <td>6.734823</td>\n",
       "      <td>8.083266</td>\n",
       "      <td>10.916053</td>\n",
       "      <td>11.515963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>-511.261424</td>\n",
       "      <td>63.589861</td>\n",
       "      <td>-30.664435</td>\n",
       "      <td>7.591162</td>\n",
       "      <td>-3.074039</td>\n",
       "      <td>-1.882242</td>\n",
       "      <td>-18.788901</td>\n",
       "      <td>-4.162927</td>\n",
       "      <td>-14.522697</td>\n",
       "      <td>-6.516644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610672</td>\n",
       "      <td>0.254156</td>\n",
       "      <td>3.653055</td>\n",
       "      <td>3.812585</td>\n",
       "      <td>2.187212</td>\n",
       "      <td>3.177447</td>\n",
       "      <td>5.212824</td>\n",
       "      <td>6.252294</td>\n",
       "      <td>11.193432</td>\n",
       "      <td>11.602554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>-405.257796</td>\n",
       "      <td>41.294204</td>\n",
       "      <td>-30.586044</td>\n",
       "      <td>-0.538227</td>\n",
       "      <td>-18.484349</td>\n",
       "      <td>-14.656285</td>\n",
       "      <td>-16.151359</td>\n",
       "      <td>-13.920813</td>\n",
       "      <td>-16.820214</td>\n",
       "      <td>3.324283</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.249055</td>\n",
       "      <td>0.077632</td>\n",
       "      <td>-0.483441</td>\n",
       "      <td>-0.436015</td>\n",
       "      <td>0.735576</td>\n",
       "      <td>1.786816</td>\n",
       "      <td>1.513320</td>\n",
       "      <td>3.222857</td>\n",
       "      <td>4.312742</td>\n",
       "      <td>5.172601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>-473.056064</td>\n",
       "      <td>53.553688</td>\n",
       "      <td>-20.607428</td>\n",
       "      <td>10.907097</td>\n",
       "      <td>-1.008979</td>\n",
       "      <td>-5.573545</td>\n",
       "      <td>-19.404833</td>\n",
       "      <td>5.218593</td>\n",
       "      <td>-28.027699</td>\n",
       "      <td>-6.464189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.640661</td>\n",
       "      <td>-2.040031</td>\n",
       "      <td>-0.220179</td>\n",
       "      <td>-0.151539</td>\n",
       "      <td>-3.876288</td>\n",
       "      <td>1.615302</td>\n",
       "      <td>4.084427</td>\n",
       "      <td>9.134243</td>\n",
       "      <td>11.374112</td>\n",
       "      <td>14.116318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>-474.610424</td>\n",
       "      <td>24.998921</td>\n",
       "      <td>-24.003816</td>\n",
       "      <td>2.117283</td>\n",
       "      <td>-15.958262</td>\n",
       "      <td>-8.645467</td>\n",
       "      <td>-19.333958</td>\n",
       "      <td>-6.487021</td>\n",
       "      <td>-26.644005</td>\n",
       "      <td>0.500758</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.764503</td>\n",
       "      <td>2.385815</td>\n",
       "      <td>-0.887787</td>\n",
       "      <td>-4.043890</td>\n",
       "      <td>-1.560124</td>\n",
       "      <td>0.833698</td>\n",
       "      <td>1.932070</td>\n",
       "      <td>-1.613096</td>\n",
       "      <td>1.777461</td>\n",
       "      <td>7.443244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>-463.701452</td>\n",
       "      <td>60.063811</td>\n",
       "      <td>-25.744653</td>\n",
       "      <td>10.314075</td>\n",
       "      <td>4.596226</td>\n",
       "      <td>-9.529221</td>\n",
       "      <td>-17.214313</td>\n",
       "      <td>-7.799369</td>\n",
       "      <td>-18.088701</td>\n",
       "      <td>-0.637278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438399</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-1.187604</td>\n",
       "      <td>-1.340231</td>\n",
       "      <td>-0.190074</td>\n",
       "      <td>1.682775</td>\n",
       "      <td>7.134923</td>\n",
       "      <td>9.182276</td>\n",
       "      <td>12.354645</td>\n",
       "      <td>11.913918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>-469.952301</td>\n",
       "      <td>24.855493</td>\n",
       "      <td>-14.537214</td>\n",
       "      <td>2.183493</td>\n",
       "      <td>-22.056347</td>\n",
       "      <td>-4.090180</td>\n",
       "      <td>-12.715464</td>\n",
       "      <td>-5.515845</td>\n",
       "      <td>-12.651356</td>\n",
       "      <td>0.111079</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.175247</td>\n",
       "      <td>1.521526</td>\n",
       "      <td>-1.151881</td>\n",
       "      <td>-1.664821</td>\n",
       "      <td>-0.822573</td>\n",
       "      <td>2.082598</td>\n",
       "      <td>1.597399</td>\n",
       "      <td>1.057190</td>\n",
       "      <td>1.596515</td>\n",
       "      <td>5.777690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2452 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5   \\\n",
       "0    -700.398878  58.630210  -3.025852  16.040243   4.248529   3.869935   \n",
       "1    -653.169017  58.028073 -12.581209  11.818785  -7.681562  -0.617142   \n",
       "2    -617.052327  60.103365  -5.984258  13.886286   1.120427   0.511750   \n",
       "3    -698.721214  47.088487 -11.333611  12.963090  -8.005652  -1.252229   \n",
       "4    -715.922663  71.976798   4.457525  18.602500   8.404048   3.621956   \n",
       "5    -739.440873  55.755429 -23.246592  13.280397  -3.838021  -5.957225   \n",
       "6    -653.982540  77.230163   1.074312  23.132712   5.029030   2.561855   \n",
       "7    -692.393582  68.048347   3.857427  26.590947   2.598323   6.667670   \n",
       "8    -741.353967  96.758128  16.994427  37.401855  17.461665  14.264806   \n",
       "9    -574.430922  56.297725  -3.995249   9.307219   0.660650   0.722855   \n",
       "10   -734.559104  66.724170  19.973302  24.432135   8.267443  10.193674   \n",
       "11   -686.793732  63.759015  -5.019265  14.655203  -2.392172  -0.812546   \n",
       "12   -737.222784  68.615129  -0.249202  14.366146   6.971780   4.769932   \n",
       "13   -765.338586  47.169830   1.476838  15.077700 -10.482029  -1.461810   \n",
       "14   -638.686598  72.511378  -5.997497  15.943515   7.175803   1.197456   \n",
       "15   -663.830633  61.380003   2.275193  12.700737  -4.215593  -4.703434   \n",
       "16   -646.566034  78.240240   5.812849  23.272988  13.461742  11.165088   \n",
       "17   -754.981614  59.807533   0.850680  15.180320  -4.610334  -0.571079   \n",
       "18   -723.215910  80.465438   8.933278  21.701921   4.516072  10.858137   \n",
       "19   -719.650165  40.265067 -10.677992  12.497934 -10.565180  -2.727621   \n",
       "20   -596.031184  69.309364   0.296093  16.458553   5.692145  -2.806871   \n",
       "21   -705.119350  59.930869  -3.234201  10.848731  -0.405465   2.574034   \n",
       "22   -647.665136  66.325564  -8.796494  13.239719   7.799501  -1.948146   \n",
       "23   -689.293025  46.676768 -10.846545   4.358803 -11.270014  -4.636991   \n",
       "24   -695.557828  59.240152  -5.372777  19.776367   5.200386   3.324629   \n",
       "25   -648.336287  52.471800 -14.081988  14.690887  -8.900879  -0.140372   \n",
       "26   -621.232796  71.126636   1.423355  20.778776   7.105571   5.159855   \n",
       "27   -701.859704  48.038854 -11.436011  10.035827 -11.794776  -4.020213   \n",
       "28   -733.987504  75.365506   0.546373  23.461105   8.043523   6.832906   \n",
       "29   -700.601184  57.836145 -23.408566  18.187676  -4.776364  -8.323164   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "2422 -429.203846  62.085675 -18.523041  14.571932  -1.515850   6.183253   \n",
       "2423 -476.160670  57.733493 -27.283853  12.225064  -8.159843  -3.457179   \n",
       "2424 -389.781932  29.680010 -34.155934  -7.672322 -25.545736 -18.100876   \n",
       "2425 -477.425029  57.416271 -22.884046  11.699640  -1.564553  -5.759070   \n",
       "2426 -463.881637  25.228983 -19.256518  -0.568383 -15.849786  -8.165686   \n",
       "2427 -467.551978  58.683119 -22.654533  12.242594   0.267150  -7.828310   \n",
       "2428 -466.656045  34.169956 -13.513101   5.190642 -20.654025  -1.952806   \n",
       "2429 -454.286043  57.456880 -28.010895   9.882527   0.564823  -1.709530   \n",
       "2430 -469.116970  24.932663 -39.038266   6.984547 -17.489141 -11.372803   \n",
       "2431 -363.731149  71.693802 -19.233360  16.502981  -4.358719  10.501375   \n",
       "2432 -468.246852  22.862166 -24.003663   5.851704 -16.788944  -7.009547   \n",
       "2433 -541.059628  52.016956  -8.113971  14.753582  -2.673453  -1.577187   \n",
       "2434 -440.992862  32.873942 -44.255752  -2.391997 -19.898011 -19.198309   \n",
       "2435 -472.640220  53.980659 -27.712259  12.405955   0.535059 -17.107110   \n",
       "2436 -495.574887  27.565998 -30.299083  10.835284 -10.216362 -15.362682   \n",
       "2437 -563.862532  64.251987  -1.875247  23.504682  11.307487   1.261256   \n",
       "2438 -518.318255  32.038844 -13.752025  10.237128  -8.979896  -8.744242   \n",
       "2439 -530.325941  58.406973  -4.335286  12.072920  -4.847376   7.940704   \n",
       "2440 -428.921656  40.096348 -28.985306  -5.392292  -9.386025 -15.318210   \n",
       "2441 -521.366716  54.325254 -18.131659  17.833857  -1.772298  -1.253668   \n",
       "2442 -550.557945  35.701355 -23.115209  11.718848 -14.158571  -8.739013   \n",
       "2443 -444.586361  47.312312 -23.933954  14.736932   3.365753  -7.494775   \n",
       "2444 -489.893832  36.523780 -28.479019  10.569031 -10.309646 -19.148973   \n",
       "2445 -462.836162  62.963434 -21.001537  10.123778  -0.078926   4.516155   \n",
       "2446 -511.261424  63.589861 -30.664435   7.591162  -3.074039  -1.882242   \n",
       "2447 -405.257796  41.294204 -30.586044  -0.538227 -18.484349 -14.656285   \n",
       "2448 -473.056064  53.553688 -20.607428  10.907097  -1.008979  -5.573545   \n",
       "2449 -474.610424  24.998921 -24.003816   2.117283 -15.958262  -8.645467   \n",
       "2450 -463.701452  60.063811 -25.744653  10.314075   4.596226  -9.529221   \n",
       "2451 -469.952301  24.855493 -14.537214   2.183493 -22.056347  -4.090180   \n",
       "\n",
       "             6          7          8          9   ...        30        31  \\\n",
       "0     -6.381717  -0.188634 -13.735003  -0.319723  ... -1.411359 -2.769772   \n",
       "1     -8.337758  -5.823571  -6.547591   1.458057  ...  0.788761  2.570493   \n",
       "2    -14.841357  -4.016369  -5.575839  -6.309851  ... -1.133367 -2.019846   \n",
       "3    -10.009068  -9.434125 -10.318875  -0.864674  ... -0.299306 -0.004270   \n",
       "4     -1.974365   0.022674  -2.678083   4.232804  ...  0.505697  0.424780   \n",
       "5    -12.653000  -6.214706 -10.689208   3.398455  ...  4.583356  5.935799   \n",
       "6    -10.727559  -3.295055  -1.869739  -0.463441  ... -4.285201 -0.391933   \n",
       "7      1.640871  -4.939541  -1.004390   1.792656  ...  1.513685  1.524577   \n",
       "8      1.034319   9.790047   3.433929   9.897499  ...  1.575394  2.135275   \n",
       "9    -10.766332  -3.891049  -7.494775   1.972640  ...  3.434052  3.164500   \n",
       "10     3.384751   5.832733  -2.836397   4.247270  ...  0.924266  0.830266   \n",
       "11    -5.175949  -6.032188 -13.602619  -1.257705  ...  0.843919  2.624703   \n",
       "12    -3.964833   0.740859  -6.925254   3.283133  ...  0.715441 -2.216875   \n",
       "13    -8.149069  -4.225049  -7.926242   0.051971  ... -0.974465 -1.076629   \n",
       "14    -1.932479   1.161894  -6.393956   1.198193  ...  0.569476 -0.153124   \n",
       "15   -11.695962  -3.269569  -8.981364  -0.019918  ...  0.993280 -0.269033   \n",
       "16     6.540210  -2.067871  -7.179390   9.426169  ... -0.517587 -0.507614   \n",
       "17    -6.452572  -9.619606 -11.002419   2.098613  ... -2.768783 -1.801866   \n",
       "18     1.185063   0.157663  -8.158168   3.154788  ...  2.799036 -0.825371   \n",
       "19   -11.100496  -7.746538  -8.434710  -1.384802  ...  2.023025  2.927179   \n",
       "20     0.449962  -1.387645 -11.512282   2.076061  ... -1.046698 -1.948009   \n",
       "21    -9.002455  -1.877024 -15.148181   1.105963  ...  1.377368  2.297907   \n",
       "22    -7.762551  -0.681861 -12.760769  -2.712420  ... -2.227505 -1.646200   \n",
       "23   -14.147990  -9.910705 -11.034352  -4.430380  ...  4.569240  6.352039   \n",
       "24    -4.668407  -2.193975 -14.508076   1.307903  ... -1.055913 -3.102514   \n",
       "25    -8.049234  -4.748140  -5.140948   0.179288  ...  1.138597  3.222392   \n",
       "26   -11.032454  -0.481391  -2.107458  -1.794385  ...  1.418735  0.628589   \n",
       "27   -11.892638 -11.610067 -13.811998  -2.650048  ...  0.344915  2.232102   \n",
       "28    -0.417122  -0.153737  -1.305530   6.911249  ...  0.693509  1.221129   \n",
       "29   -13.772667  -5.920182  -7.138942   4.960144  ...  4.195435  5.989556   \n",
       "...         ...        ...        ...        ...  ...       ...       ...   \n",
       "2422  -8.046456  -1.481375 -17.532570   2.494749  ...  2.764888  2.381522   \n",
       "2423 -18.587796 -10.986997 -20.024006  -2.851773  ...  4.372591  2.839557   \n",
       "2424 -14.895015  -9.886096 -15.466568   0.828662  ... -1.102575 -0.868613   \n",
       "2425 -18.075790   3.671213 -28.971663  -2.015684  ...  1.385775 -1.550216   \n",
       "2426 -18.913574  -5.280563 -26.643877  -1.343729  ... -0.916535  2.345366   \n",
       "2427 -16.688125  -4.233726 -16.579934  -1.539022  ... -1.486852 -1.767214   \n",
       "2428 -13.543329  -4.892323 -12.826017   2.937352  ... -2.040943 -2.017374   \n",
       "2429 -20.537823 -10.167647 -25.669589  -4.336482  ...  0.095318 -0.348905   \n",
       "2430 -10.220135 -14.558235 -15.385056   2.714475  ...  0.475706  2.433466   \n",
       "2431 -15.280219  -8.005958 -11.037171  -5.284053  ...  4.400745  6.690452   \n",
       "2432  -9.375116 -15.625277 -12.176636  -0.531862  ... -1.284645  2.262799   \n",
       "2433 -14.328500   4.092199 -12.480850   0.101191  ...  0.825417  1.013287   \n",
       "2434 -17.034746  -8.098862 -17.049275  11.658251  ...  1.799569  5.429992   \n",
       "2435 -16.421782  -2.927878 -16.100153  -0.669792  ... -3.144668  3.093919   \n",
       "2436  -4.401279 -12.859067 -14.968482   4.055844  ...  0.753993  4.940733   \n",
       "2437 -13.603296   3.872112 -13.415906  -1.189780  ...  1.547678  2.703273   \n",
       "2438 -14.966661 -14.077097 -14.856027  -0.402052  ...  1.870467  2.136834   \n",
       "2439  -8.287180  -2.737741 -14.531733  -1.755857  ...  0.182924  0.326268   \n",
       "2440 -16.408057 -11.023748 -25.511774   9.051820  ...  4.320986  2.382091   \n",
       "2441 -20.517578   0.791653 -17.416759   1.175464  ... -2.353009  1.468018   \n",
       "2442  -9.842138 -10.146640  -8.128038   0.225512  ... -2.280342  0.386437   \n",
       "2443 -14.385962   0.643082 -22.092245   4.009877  ... -0.088949  0.851614   \n",
       "2444 -17.317035 -13.192929 -18.663201   5.794672  ... -3.458641  1.181090   \n",
       "2445  -8.090581  -7.394699 -19.407525   2.447414  ...  1.861383  1.817470   \n",
       "2446 -18.788901  -4.162927 -14.522697  -6.516644  ...  0.610672  0.254156   \n",
       "2447 -16.151359 -13.920813 -16.820214   3.324283  ... -1.249055  0.077632   \n",
       "2448 -19.404833   5.218593 -28.027699  -6.464189  ...  1.640661 -2.040031   \n",
       "2449 -19.333958  -6.487021 -26.644005   0.500758  ... -1.764503  2.385815   \n",
       "2450 -17.214313  -7.799369 -18.088701  -0.637278  ... -0.438399 -0.092734   \n",
       "2451 -12.715464  -5.515845 -12.651356   0.111079  ... -2.175247  1.521526   \n",
       "\n",
       "            32        33        34        35        36        37         38  \\\n",
       "0    -2.042008 -2.522663 -2.507449 -2.250499 -0.381507 -2.481059  -2.791022   \n",
       "1     2.558635  3.708506  2.790020  2.201920 -1.021457  0.819200  -0.277811   \n",
       "2    -1.024787  0.331097  0.531833 -1.621018 -2.158077 -2.502774  -0.676132   \n",
       "3    -1.408215  0.958173  2.574661  1.877765  1.608618  0.222401   4.338524   \n",
       "4     0.972097  0.210004  0.667769  1.918957  0.174543 -1.768463  -1.512281   \n",
       "5     4.206937  5.363243  4.332376  3.667531  1.550318 -0.558750   1.737009   \n",
       "6    -1.683185 -1.358684 -2.281433 -3.152291 -5.220386 -3.469335  -1.696725   \n",
       "7     2.985039  5.605745  5.896914  5.154771  4.143033  5.628289   4.912781   \n",
       "8     1.080048  0.077569  1.954016  0.924915 -0.342838 -1.029033  -1.741040   \n",
       "9     5.605457  3.372160  5.965045  2.542946  2.380176  1.891932   1.244445   \n",
       "10    0.951789  0.409593 -0.970139 -0.998077 -0.801911 -1.194514  -1.572553   \n",
       "11    3.302091  4.932687  5.131727  3.666039  4.504458  1.554605   0.302437   \n",
       "12    0.222590  0.218809  0.430849  0.554459 -0.558221 -1.556306  -1.882601   \n",
       "13   -0.515905  0.504372  1.259402  2.089944  3.679059  2.809832   1.927483   \n",
       "14    0.349048 -0.381267 -0.498064  0.580211 -0.288043  1.125915   0.291572   \n",
       "15    0.044593 -0.209263 -0.497776  0.500730  1.943027  2.300725   2.240043   \n",
       "16    1.068280 -0.431788 -0.993862 -1.745499 -0.998088 -1.758631  -1.363380   \n",
       "17   -3.324028 -1.853533 -1.035654  0.965440  4.182645  7.387866  10.678265   \n",
       "18   -0.517987  1.425363 -0.862558  1.228814 -0.960936 -2.644739  -2.159275   \n",
       "19    2.053068  2.236037  2.297235  3.286657  2.094471  1.377533  -1.344257   \n",
       "20   -2.145055 -1.342209 -1.250720 -0.920083 -2.509349 -2.264665  -1.518574   \n",
       "21   -0.597607  1.191020  1.685758  4.253395  5.946149  6.300133   2.370302   \n",
       "22   -2.824971 -1.618364 -2.738211 -0.041697 -0.445510 -0.147109  -0.443362   \n",
       "23    7.300994  5.808718  4.554309  1.843078  0.787600  0.603052   0.108461   \n",
       "24   -1.697880 -2.922661 -2.544465 -1.289832 -0.797254 -3.586074  -2.706395   \n",
       "25    3.672537  3.186997  2.064213  2.999678 -0.035473  1.505137  -0.099722   \n",
       "26    0.983358  1.843995  2.002833  1.518636  1.326444  1.038551   1.285298   \n",
       "27    2.180859 -0.205000 -1.200028 -1.141648  1.193933 -0.171482   2.330363   \n",
       "28    0.807871  0.241845  0.011237  2.517624  1.372746 -1.389155  -2.925384   \n",
       "29    5.457668  8.291785  8.284182  7.364994  3.067757 -0.109833   3.187426   \n",
       "...        ...       ...       ...       ...       ...       ...        ...   \n",
       "2422  4.564110  4.151742  4.128974  3.486980  6.217521  6.614717   8.534979   \n",
       "2423  5.506363  7.651137  6.603731  5.822065  5.694686  2.824640   5.872091   \n",
       "2424 -1.742941 -2.597116 -0.986093  0.337788  0.774260  0.300542   5.688090   \n",
       "2425  1.814287 -0.701365 -4.168486  1.610681  2.516717  6.146086   8.880870   \n",
       "2426 -0.727863 -4.293114 -1.566936  0.033171  1.065494 -1.023938   1.883473   \n",
       "2427 -1.389356 -0.648048 -0.599664  0.738672  3.030808  5.280357   9.240118   \n",
       "2428 -1.707237 -1.737038 -0.366111  1.945852  1.262407  2.308048   2.737007   \n",
       "2429  0.913677 -0.077920 -0.197528  2.560753  5.958482  9.160935   9.730867   \n",
       "2430 -2.165421 -4.516422 -1.098824  3.480164 -0.090849 -0.770057   6.024706   \n",
       "2431  4.785794  3.915019  3.445365  3.222094  5.648620  9.422134   9.478660   \n",
       "2432 -0.304973 -5.778587 -1.170822  0.324805  0.087237 -0.856777   3.920226   \n",
       "2433  2.891954  2.667731  2.843483  2.167473  4.230566  6.786861  10.595272   \n",
       "2434  3.519748  3.263165  1.389722  3.486272  1.296781  1.221333   4.486672   \n",
       "2435  2.414802  4.939812  3.184422  4.305104  2.778225  5.659955   7.980364   \n",
       "2436  1.890999  0.468223  2.349840  5.618784  6.002854  2.441198   6.244189   \n",
       "2437  2.300020  1.774639  3.111295  3.451707  5.322471  4.609474   6.939531   \n",
       "2438 -2.737673 -3.423134 -2.481716  2.306540 -1.143341  1.014231   5.708663   \n",
       "2439  3.425445  0.514945 -0.334071 -0.168194  1.996263  6.399669  12.176999   \n",
       "2440 -2.431258 -1.943195  1.365404  3.361830  2.462617  1.676635   3.234639   \n",
       "2441 -1.404139  1.529970  2.077743  1.659654  2.243726  3.093645   8.210729   \n",
       "2442 -2.974271  0.380171  0.549122  0.822314  0.812847  1.695049   1.728104   \n",
       "2443  2.509858  1.908783  3.344266  2.556932  5.009627  5.761484   9.606792   \n",
       "2444 -1.933176 -6.533607 -4.179962  0.479534  2.609350 -2.024513   2.703920   \n",
       "2445  6.067732  3.764157  4.073399  2.446536  6.734823  8.083266  10.916053   \n",
       "2446  3.653055  3.812585  2.187212  3.177447  5.212824  6.252294  11.193432   \n",
       "2447 -0.483441 -0.436015  0.735576  1.786816  1.513320  3.222857   4.312742   \n",
       "2448 -0.220179 -0.151539 -3.876288  1.615302  4.084427  9.134243  11.374112   \n",
       "2449 -0.887787 -4.043890 -1.560124  0.833698  1.932070 -1.613096   1.777461   \n",
       "2450 -1.187604 -1.340231 -0.190074  1.682775  7.134923  9.182276  12.354645   \n",
       "2451 -1.151881 -1.664821 -0.822573  2.082598  1.597399  1.057190   1.596515   \n",
       "\n",
       "             39  \n",
       "0     -2.244866  \n",
       "1      0.207586  \n",
       "2      2.089097  \n",
       "3      4.266795  \n",
       "4     -0.253280  \n",
       "5      3.253290  \n",
       "6     -0.342364  \n",
       "7      4.313900  \n",
       "8     -0.339235  \n",
       "9      3.589855  \n",
       "10    -0.679407  \n",
       "11    -2.583656  \n",
       "12    -0.562863  \n",
       "13     1.177062  \n",
       "14     0.333098  \n",
       "15     1.939023  \n",
       "16    -2.398707  \n",
       "17    10.405077  \n",
       "18    -0.943426  \n",
       "19    -1.004930  \n",
       "20    -2.511123  \n",
       "21     4.118457  \n",
       "22     0.124879  \n",
       "23    -0.394463  \n",
       "24    -2.812933  \n",
       "25    -0.642433  \n",
       "26     1.143829  \n",
       "27     4.330539  \n",
       "28    -0.651365  \n",
       "29     6.803388  \n",
       "...         ...  \n",
       "2422   8.920184  \n",
       "2423   8.519549  \n",
       "2424   3.684110  \n",
       "2425  12.728390  \n",
       "2426   6.978850  \n",
       "2427  11.328468  \n",
       "2428   2.305828  \n",
       "2429   8.572048  \n",
       "2430   8.801284  \n",
       "2431  10.289215  \n",
       "2432   6.802940  \n",
       "2433  13.427793  \n",
       "2434   5.342615  \n",
       "2435   7.617889  \n",
       "2436   8.364046  \n",
       "2437   6.491947  \n",
       "2438   9.272492  \n",
       "2439  11.788868  \n",
       "2440   2.727310  \n",
       "2441  10.980291  \n",
       "2442   0.802639  \n",
       "2443  10.331644  \n",
       "2444   9.721113  \n",
       "2445  11.515963  \n",
       "2446  11.602554  \n",
       "2447   5.172601  \n",
       "2448  14.116318  \n",
       "2449   7.443244  \n",
       "2450  11.913918  \n",
       "2451   5.777690  \n",
       "\n",
       "[2452 rows x 40 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ravdess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "\n",
    "\n",
    "\n",
    "path = '/Users/ioann/saveelist/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "      \n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_features = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not the whole radvess cause its a lot bigger\n",
    "X_train=ravdess_features[:1700]\n",
    "\n",
    "X_test=savee_features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "y_train=labels_ravdess[:1700]\n",
    "y_test=labels_savee\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 7)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 40, 256)           1536      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 40, 128)           163968    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 4487      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 334,087\n",
      "Trainable params: 334,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ioann\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1700 samples, validate on 480 samples\n",
      "Epoch 1/700\n",
      "1700/1700 [==============================] - 1s 780us/step - loss: 2.1926 - accuracy: 0.2653 - val_loss: 1.9130 - val_accuracy: 0.2708\n",
      "Epoch 2/700\n",
      "1700/1700 [==============================] - 1s 610us/step - loss: 1.9117 - accuracy: 0.3012 - val_loss: 1.9585 - val_accuracy: 0.2562\n",
      "Epoch 3/700\n",
      "1700/1700 [==============================] - 1s 613us/step - loss: 1.8182 - accuracy: 0.3282 - val_loss: 1.9756 - val_accuracy: 0.2500\n",
      "Epoch 4/700\n",
      "1700/1700 [==============================] - 1s 612us/step - loss: 1.7472 - accuracy: 0.3347 - val_loss: 1.9962 - val_accuracy: 0.2500\n",
      "Epoch 5/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.7091 - accuracy: 0.3429 - val_loss: 1.9426 - val_accuracy: 0.2729\n",
      "Epoch 6/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.6564 - accuracy: 0.3606 - val_loss: 2.0038 - val_accuracy: 0.2542\n",
      "Epoch 7/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.6365 - accuracy: 0.3500 - val_loss: 2.1484 - val_accuracy: 0.2521\n",
      "Epoch 8/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 1.5914 - accuracy: 0.3765 - val_loss: 2.0135 - val_accuracy: 0.2542\n",
      "Epoch 9/700\n",
      "1700/1700 [==============================] - 1s 612us/step - loss: 1.5607 - accuracy: 0.4006 - val_loss: 2.1071 - val_accuracy: 0.2542\n",
      "Epoch 10/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 1.5471 - accuracy: 0.4041 - val_loss: 2.2654 - val_accuracy: 0.2521\n",
      "Epoch 11/700\n",
      "1700/1700 [==============================] - 1s 668us/step - loss: 1.5368 - accuracy: 0.4024 - val_loss: 2.1451 - val_accuracy: 0.2542\n",
      "Epoch 12/700\n",
      "1700/1700 [==============================] - 1s 645us/step - loss: 1.5213 - accuracy: 0.4129 - val_loss: 2.0641 - val_accuracy: 0.2667\n",
      "Epoch 13/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 1.5089 - accuracy: 0.4188 - val_loss: 2.4033 - val_accuracy: 0.2542\n",
      "Epoch 14/700\n",
      "1700/1700 [==============================] - 1s 611us/step - loss: 1.4926 - accuracy: 0.4365 - val_loss: 2.1686 - val_accuracy: 0.2604\n",
      "Epoch 15/700\n",
      "1700/1700 [==============================] - 1s 614us/step - loss: 1.4792 - accuracy: 0.4359 - val_loss: 2.2144 - val_accuracy: 0.2521\n",
      "Epoch 16/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 1.4826 - accuracy: 0.4247 - val_loss: 2.2176 - val_accuracy: 0.2604\n",
      "Epoch 17/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 1.4506 - accuracy: 0.4382 - val_loss: 2.2084 - val_accuracy: 0.2646\n",
      "Epoch 18/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 1.4549 - accuracy: 0.4359 - val_loss: 2.0426 - val_accuracy: 0.2792\n",
      "Epoch 19/700\n",
      "1700/1700 [==============================] - 1s 604us/step - loss: 1.4467 - accuracy: 0.4465 - val_loss: 2.3243 - val_accuracy: 0.2479\n",
      "Epoch 20/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.4401 - accuracy: 0.4476 - val_loss: 2.0761 - val_accuracy: 0.2562\n",
      "Epoch 21/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 1.4332 - accuracy: 0.4476 - val_loss: 2.3071 - val_accuracy: 0.2688\n",
      "Epoch 22/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.4246 - accuracy: 0.4576 - val_loss: 2.2519 - val_accuracy: 0.2458\n",
      "Epoch 23/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.4183 - accuracy: 0.4512 - val_loss: 2.1961 - val_accuracy: 0.2562\n",
      "Epoch 24/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 1.4130 - accuracy: 0.4641 - val_loss: 2.2077 - val_accuracy: 0.2688\n",
      "Epoch 25/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.3983 - accuracy: 0.4659 - val_loss: 2.2180 - val_accuracy: 0.2667\n",
      "Epoch 26/700\n",
      "1700/1700 [==============================] - 1s 618us/step - loss: 1.3979 - accuracy: 0.4718 - val_loss: 2.4667 - val_accuracy: 0.2646\n",
      "Epoch 27/700\n",
      "1700/1700 [==============================] - 1s 636us/step - loss: 1.3880 - accuracy: 0.4747 - val_loss: 2.3287 - val_accuracy: 0.2688\n",
      "Epoch 28/700\n",
      "1700/1700 [==============================] - 1s 612us/step - loss: 1.3903 - accuracy: 0.4682 - val_loss: 2.4302 - val_accuracy: 0.2542\n",
      "Epoch 29/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.3836 - accuracy: 0.4759 - val_loss: 2.2287 - val_accuracy: 0.2708\n",
      "Epoch 30/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.3764 - accuracy: 0.4818 - val_loss: 2.3395 - val_accuracy: 0.2583\n",
      "Epoch 31/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 1.3678 - accuracy: 0.4782 - val_loss: 2.4147 - val_accuracy: 0.2729\n",
      "Epoch 32/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 1.3579 - accuracy: 0.4906 - val_loss: 2.1462 - val_accuracy: 0.2625\n",
      "Epoch 33/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 1.3497 - accuracy: 0.4906 - val_loss: 2.1578 - val_accuracy: 0.2750\n",
      "Epoch 34/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 1.3512 - accuracy: 0.5006 - val_loss: 2.2417 - val_accuracy: 0.2729\n",
      "Epoch 35/700\n",
      "1700/1700 [==============================] - 1s 611us/step - loss: 1.3479 - accuracy: 0.4906 - val_loss: 2.1497 - val_accuracy: 0.2625\n",
      "Epoch 36/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 1.3405 - accuracy: 0.5024 - val_loss: 2.3014 - val_accuracy: 0.2729\n",
      "Epoch 37/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 1.3272 - accuracy: 0.5053 - val_loss: 2.1565 - val_accuracy: 0.2729\n",
      "Epoch 38/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 1.3279 - accuracy: 0.5071 - val_loss: 2.4115 - val_accuracy: 0.2708\n",
      "Epoch 39/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.3326 - accuracy: 0.5047 - val_loss: 2.4614 - val_accuracy: 0.2646\n",
      "Epoch 40/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 1.3136 - accuracy: 0.5159 - val_loss: 2.2685 - val_accuracy: 0.2667\n",
      "Epoch 41/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.3084 - accuracy: 0.5059 - val_loss: 2.3315 - val_accuracy: 0.2729\n",
      "Epoch 42/700\n",
      "1700/1700 [==============================] - 1s 641us/step - loss: 1.3119 - accuracy: 0.5094 - val_loss: 2.1944 - val_accuracy: 0.2688\n",
      "Epoch 43/700\n",
      "1700/1700 [==============================] - 1s 632us/step - loss: 1.3009 - accuracy: 0.5094 - val_loss: 2.3097 - val_accuracy: 0.2438\n",
      "Epoch 44/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 1.2933 - accuracy: 0.5194 - val_loss: 2.5298 - val_accuracy: 0.2625\n",
      "Epoch 45/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 1.2922 - accuracy: 0.5171 - val_loss: 2.3456 - val_accuracy: 0.2771\n",
      "Epoch 46/700\n",
      "1700/1700 [==============================] - 1s 604us/step - loss: 1.2904 - accuracy: 0.5259 - val_loss: 2.3362 - val_accuracy: 0.2688\n",
      "Epoch 47/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 1.2773 - accuracy: 0.5235 - val_loss: 2.3056 - val_accuracy: 0.2708\n",
      "Epoch 48/700\n",
      "1700/1700 [==============================] - 1s 606us/step - loss: 1.2757 - accuracy: 0.5241 - val_loss: 2.4509 - val_accuracy: 0.2521\n",
      "Epoch 49/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 1.2716 - accuracy: 0.5253 - val_loss: 2.4340 - val_accuracy: 0.2667\n",
      "Epoch 50/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.2662 - accuracy: 0.5312 - val_loss: 2.2425 - val_accuracy: 0.2625\n",
      "Epoch 51/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.2617 - accuracy: 0.5341 - val_loss: 2.5365 - val_accuracy: 0.2625\n",
      "Epoch 52/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 1.2540 - accuracy: 0.5294 - val_loss: 2.3541 - val_accuracy: 0.2500\n",
      "Epoch 53/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 1.2524 - accuracy: 0.5388 - val_loss: 2.3012 - val_accuracy: 0.2458\n",
      "Epoch 54/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.2413 - accuracy: 0.5465 - val_loss: 2.4996 - val_accuracy: 0.2646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 1.2310 - accuracy: 0.5435 - val_loss: 2.3257 - val_accuracy: 0.2604\n",
      "Epoch 56/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.2402 - accuracy: 0.5424 - val_loss: 2.6209 - val_accuracy: 0.2604\n",
      "Epoch 57/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 1.2373 - accuracy: 0.5400 - val_loss: 2.3949 - val_accuracy: 0.2458\n",
      "Epoch 58/700\n",
      "1700/1700 [==============================] - 1s 629us/step - loss: 1.2216 - accuracy: 0.5476 - val_loss: 2.7055 - val_accuracy: 0.2562\n",
      "Epoch 59/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 1.2287 - accuracy: 0.5494 - val_loss: 2.5235 - val_accuracy: 0.2458\n",
      "Epoch 60/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 1.2141 - accuracy: 0.5588 - val_loss: 2.5819 - val_accuracy: 0.2604\n",
      "Epoch 61/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 1.2094 - accuracy: 0.5518 - val_loss: 2.5714 - val_accuracy: 0.2542\n",
      "Epoch 62/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 1.2035 - accuracy: 0.5588 - val_loss: 2.6350 - val_accuracy: 0.2583\n",
      "Epoch 63/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 1.2140 - accuracy: 0.5429 - val_loss: 2.5142 - val_accuracy: 0.2417\n",
      "Epoch 64/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 1.1946 - accuracy: 0.5594 - val_loss: 2.5505 - val_accuracy: 0.2625\n",
      "Epoch 65/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.1971 - accuracy: 0.5576 - val_loss: 2.5985 - val_accuracy: 0.2562\n",
      "Epoch 66/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 1.1881 - accuracy: 0.5694 - val_loss: 2.5105 - val_accuracy: 0.2625\n",
      "Epoch 67/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1844 - accuracy: 0.5665 - val_loss: 2.4037 - val_accuracy: 0.2500\n",
      "Epoch 68/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1839 - accuracy: 0.5600 - val_loss: 2.5266 - val_accuracy: 0.2604\n",
      "Epoch 69/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1787 - accuracy: 0.5747 - val_loss: 2.6891 - val_accuracy: 0.2458\n",
      "Epoch 70/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 1.1700 - accuracy: 0.5771 - val_loss: 2.6989 - val_accuracy: 0.2396\n",
      "Epoch 71/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 1.1700 - accuracy: 0.5871 - val_loss: 2.5423 - val_accuracy: 0.2417\n",
      "Epoch 72/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.1647 - accuracy: 0.5700 - val_loss: 2.6830 - val_accuracy: 0.2604\n",
      "Epoch 73/700\n",
      "1700/1700 [==============================] - 1s 624us/step - loss: 1.1584 - accuracy: 0.5853 - val_loss: 2.4009 - val_accuracy: 0.2438\n",
      "Epoch 74/700\n",
      "1700/1700 [==============================] - 1s 623us/step - loss: 1.1586 - accuracy: 0.5694 - val_loss: 2.7086 - val_accuracy: 0.2583\n",
      "Epoch 75/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 1.1552 - accuracy: 0.5776 - val_loss: 2.5569 - val_accuracy: 0.2313\n",
      "Epoch 76/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 1.1408 - accuracy: 0.5818 - val_loss: 2.5172 - val_accuracy: 0.2417\n",
      "Epoch 77/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 1.1353 - accuracy: 0.5818 - val_loss: 2.7427 - val_accuracy: 0.2604\n",
      "Epoch 78/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1372 - accuracy: 0.5841 - val_loss: 2.5355 - val_accuracy: 0.2333\n",
      "Epoch 79/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.1318 - accuracy: 0.5894 - val_loss: 2.6253 - val_accuracy: 0.2417\n",
      "Epoch 80/700\n",
      "1700/1700 [==============================] - 1s 603us/step - loss: 1.1274 - accuracy: 0.5929 - val_loss: 2.5809 - val_accuracy: 0.2229\n",
      "Epoch 81/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 1.1403 - accuracy: 0.5829 - val_loss: 2.5985 - val_accuracy: 0.2313\n",
      "Epoch 82/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 1.1275 - accuracy: 0.5924 - val_loss: 2.5089 - val_accuracy: 0.2292\n",
      "Epoch 83/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1202 - accuracy: 0.5741 - val_loss: 2.5665 - val_accuracy: 0.2271\n",
      "Epoch 84/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1134 - accuracy: 0.5806 - val_loss: 2.9059 - val_accuracy: 0.2521\n",
      "Epoch 85/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1151 - accuracy: 0.5994 - val_loss: 2.8608 - val_accuracy: 0.2542\n",
      "Epoch 86/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.1046 - accuracy: 0.6018 - val_loss: 2.7203 - val_accuracy: 0.2542\n",
      "Epoch 87/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 1.1027 - accuracy: 0.6024 - val_loss: 2.6834 - val_accuracy: 0.2562\n",
      "Epoch 88/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.1005 - accuracy: 0.6053 - val_loss: 2.7564 - val_accuracy: 0.1917\n",
      "Epoch 89/700\n",
      "1700/1700 [==============================] - 1s 642us/step - loss: 1.1000 - accuracy: 0.5924 - val_loss: 2.8426 - val_accuracy: 0.2521\n",
      "Epoch 90/700\n",
      "1700/1700 [==============================] - 1s 612us/step - loss: 1.0974 - accuracy: 0.5924 - val_loss: 2.8359 - val_accuracy: 0.2562\n",
      "Epoch 91/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.0902 - accuracy: 0.6024 - val_loss: 2.8663 - val_accuracy: 0.2271\n",
      "Epoch 92/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.0858 - accuracy: 0.6112 - val_loss: 2.8005 - val_accuracy: 0.2521\n",
      "Epoch 93/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.0822 - accuracy: 0.6147 - val_loss: 2.6613 - val_accuracy: 0.2250\n",
      "Epoch 94/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.0830 - accuracy: 0.6047 - val_loss: 2.6333 - val_accuracy: 0.2271\n",
      "Epoch 95/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.0822 - accuracy: 0.6065 - val_loss: 2.8989 - val_accuracy: 0.2542\n",
      "Epoch 96/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 1.0698 - accuracy: 0.6094 - val_loss: 2.6911 - val_accuracy: 0.2313\n",
      "Epoch 97/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 1.0677 - accuracy: 0.6106 - val_loss: 3.0600 - val_accuracy: 0.2562\n",
      "Epoch 98/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.0670 - accuracy: 0.6129 - val_loss: 2.8340 - val_accuracy: 0.2458\n",
      "Epoch 99/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.0607 - accuracy: 0.6194 - val_loss: 2.8051 - val_accuracy: 0.2604\n",
      "Epoch 100/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 1.0577 - accuracy: 0.6182 - val_loss: 2.8080 - val_accuracy: 0.2292\n",
      "Epoch 101/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.0603 - accuracy: 0.6200 - val_loss: 3.0575 - val_accuracy: 0.2479\n",
      "Epoch 102/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 1.0551 - accuracy: 0.6212 - val_loss: 2.8446 - val_accuracy: 0.2438\n",
      "Epoch 103/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 1.0546 - accuracy: 0.6265 - val_loss: 2.8715 - val_accuracy: 0.2479\n",
      "Epoch 104/700\n",
      "1700/1700 [==============================] - 1s 614us/step - loss: 1.0443 - accuracy: 0.6159 - val_loss: 2.9906 - val_accuracy: 0.2521\n",
      "Epoch 105/700\n",
      "1700/1700 [==============================] - 1s 657us/step - loss: 1.0341 - accuracy: 0.6182 - val_loss: 2.8775 - val_accuracy: 0.2521\n",
      "Epoch 106/700\n",
      "1700/1700 [==============================] - 1s 631us/step - loss: 1.0461 - accuracy: 0.6153 - val_loss: 2.7254 - val_accuracy: 0.2354\n",
      "Epoch 107/700\n",
      "1700/1700 [==============================] - 1s 646us/step - loss: 1.0259 - accuracy: 0.6324 - val_loss: 3.0330 - val_accuracy: 0.2250\n",
      "Epoch 108/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 1.0353 - accuracy: 0.6318 - val_loss: 3.0277 - val_accuracy: 0.2229\n",
      "Epoch 109/700\n",
      "1700/1700 [==============================] - 1s 604us/step - loss: 1.0379 - accuracy: 0.6300 - val_loss: 3.0289 - val_accuracy: 0.2208\n",
      "Epoch 110/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 595us/step - loss: 1.0251 - accuracy: 0.6247 - val_loss: 2.9359 - val_accuracy: 0.2333\n",
      "Epoch 111/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 1.0307 - accuracy: 0.6265 - val_loss: 2.9910 - val_accuracy: 0.2375\n",
      "Epoch 112/700\n",
      "1700/1700 [==============================] - 1s 606us/step - loss: 1.0237 - accuracy: 0.6294 - val_loss: 3.0660 - val_accuracy: 0.2333\n",
      "Epoch 113/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 1.0203 - accuracy: 0.6359 - val_loss: 3.1411 - val_accuracy: 0.2271\n",
      "Epoch 114/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 1.0069 - accuracy: 0.6288 - val_loss: 3.0241 - val_accuracy: 0.2479\n",
      "Epoch 115/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 1.0018 - accuracy: 0.6376 - val_loss: 3.2096 - val_accuracy: 0.2146\n",
      "Epoch 116/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 1.0064 - accuracy: 0.6365 - val_loss: 2.9446 - val_accuracy: 0.2521\n",
      "Epoch 117/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 1.0013 - accuracy: 0.6353 - val_loss: 3.1554 - val_accuracy: 0.2479\n",
      "Epoch 118/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.9959 - accuracy: 0.6435 - val_loss: 2.8170 - val_accuracy: 0.2333\n",
      "Epoch 119/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.9916 - accuracy: 0.6400 - val_loss: 3.1597 - val_accuracy: 0.2542\n",
      "Epoch 120/700\n",
      "1700/1700 [==============================] - 1s 628us/step - loss: 0.9941 - accuracy: 0.6418 - val_loss: 3.2079 - val_accuracy: 0.2542\n",
      "Epoch 121/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 0.9896 - accuracy: 0.6347 - val_loss: 3.2482 - val_accuracy: 0.2292\n",
      "Epoch 122/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 1.0019 - accuracy: 0.6488 - val_loss: 3.0869 - val_accuracy: 0.2479\n",
      "Epoch 123/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.9897 - accuracy: 0.6341 - val_loss: 2.9265 - val_accuracy: 0.2271\n",
      "Epoch 124/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.9900 - accuracy: 0.6465 - val_loss: 3.0465 - val_accuracy: 0.2292\n",
      "Epoch 125/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.9778 - accuracy: 0.6512 - val_loss: 3.0076 - val_accuracy: 0.2375\n",
      "Epoch 126/700\n",
      "1700/1700 [==============================] - 1s 586us/step - loss: 0.9792 - accuracy: 0.6482 - val_loss: 3.1730 - val_accuracy: 0.2208\n",
      "Epoch 127/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.9767 - accuracy: 0.6447 - val_loss: 3.1104 - val_accuracy: 0.2292\n",
      "Epoch 128/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.9718 - accuracy: 0.6388 - val_loss: 3.3205 - val_accuracy: 0.2396\n",
      "Epoch 129/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.9635 - accuracy: 0.6518 - val_loss: 3.1976 - val_accuracy: 0.2396\n",
      "Epoch 130/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.9680 - accuracy: 0.6571 - val_loss: 3.0171 - val_accuracy: 0.2292\n",
      "Epoch 131/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.9677 - accuracy: 0.6500 - val_loss: 3.4569 - val_accuracy: 0.2542\n",
      "Epoch 132/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.9671 - accuracy: 0.6541 - val_loss: 3.1968 - val_accuracy: 0.2396\n",
      "Epoch 133/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.9593 - accuracy: 0.6494 - val_loss: 3.1163 - val_accuracy: 0.2333\n",
      "Epoch 134/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.9559 - accuracy: 0.6465 - val_loss: 2.9497 - val_accuracy: 0.2188\n",
      "Epoch 135/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.9579 - accuracy: 0.6518 - val_loss: 3.5127 - val_accuracy: 0.2375\n",
      "Epoch 136/700\n",
      "1700/1700 [==============================] - 1s 633us/step - loss: 0.9525 - accuracy: 0.6641 - val_loss: 3.4404 - val_accuracy: 0.2542\n",
      "Epoch 137/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.9477 - accuracy: 0.6600 - val_loss: 3.6205 - val_accuracy: 0.2479\n",
      "Epoch 138/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.9367 - accuracy: 0.6700 - val_loss: 3.2441 - val_accuracy: 0.2354\n",
      "Epoch 139/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.9447 - accuracy: 0.6594 - val_loss: 3.1655 - val_accuracy: 0.2354\n",
      "Epoch 140/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.9395 - accuracy: 0.6594 - val_loss: 3.1100 - val_accuracy: 0.2396\n",
      "Epoch 141/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.9404 - accuracy: 0.6565 - val_loss: 3.3432 - val_accuracy: 0.2562\n",
      "Epoch 142/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.9378 - accuracy: 0.6682 - val_loss: 3.0278 - val_accuracy: 0.2188\n",
      "Epoch 143/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.9279 - accuracy: 0.6653 - val_loss: 3.4888 - val_accuracy: 0.2458\n",
      "Epoch 144/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.9325 - accuracy: 0.6665 - val_loss: 2.9130 - val_accuracy: 0.2229\n",
      "Epoch 145/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.9310 - accuracy: 0.6612 - val_loss: 3.5871 - val_accuracy: 0.2396\n",
      "Epoch 146/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.9203 - accuracy: 0.6782 - val_loss: 3.2189 - val_accuracy: 0.1896\n",
      "Epoch 147/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.9348 - accuracy: 0.6612 - val_loss: 3.4603 - val_accuracy: 0.2042\n",
      "Epoch 148/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.9273 - accuracy: 0.6688 - val_loss: 3.4222 - val_accuracy: 0.2083\n",
      "Epoch 149/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.9197 - accuracy: 0.6676 - val_loss: 3.1803 - val_accuracy: 0.2188\n",
      "Epoch 150/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.9177 - accuracy: 0.6747 - val_loss: 3.1987 - val_accuracy: 0.2188\n",
      "Epoch 151/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 0.9089 - accuracy: 0.6794 - val_loss: 3.2809 - val_accuracy: 0.2208\n",
      "Epoch 152/700\n",
      "1700/1700 [==============================] - 1s 639us/step - loss: 0.9157 - accuracy: 0.6729 - val_loss: 3.5008 - val_accuracy: 0.2396\n",
      "Epoch 153/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.9183 - accuracy: 0.6853 - val_loss: 3.3536 - val_accuracy: 0.2167\n",
      "Epoch 154/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.9016 - accuracy: 0.6776 - val_loss: 3.8078 - val_accuracy: 0.2333\n",
      "Epoch 155/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.8987 - accuracy: 0.6788 - val_loss: 3.3997 - val_accuracy: 0.2313\n",
      "Epoch 156/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.9022 - accuracy: 0.6800 - val_loss: 3.7496 - val_accuracy: 0.2521\n",
      "Epoch 157/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.9006 - accuracy: 0.6741 - val_loss: 3.3690 - val_accuracy: 0.2188\n",
      "Epoch 158/700\n",
      "1700/1700 [==============================] - 1s 611us/step - loss: 0.8990 - accuracy: 0.6812 - val_loss: 3.3237 - val_accuracy: 0.2208\n",
      "Epoch 159/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.8993 - accuracy: 0.6788 - val_loss: 3.4462 - val_accuracy: 0.2062\n",
      "Epoch 160/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.8894 - accuracy: 0.6865 - val_loss: 3.3569 - val_accuracy: 0.2354\n",
      "Epoch 161/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8967 - accuracy: 0.6812 - val_loss: 3.3565 - val_accuracy: 0.2229\n",
      "Epoch 162/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.8871 - accuracy: 0.6759 - val_loss: 3.4433 - val_accuracy: 0.2313\n",
      "Epoch 163/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.8834 - accuracy: 0.6865 - val_loss: 3.7034 - val_accuracy: 0.2229\n",
      "Epoch 164/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.8854 - accuracy: 0.6888 - val_loss: 3.6745 - val_accuracy: 0.2354\n",
      "Epoch 165/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.8820 - accuracy: 0.6771 - val_loss: 3.3696 - val_accuracy: 0.2208\n",
      "Epoch 166/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8797 - accuracy: 0.6947 - val_loss: 3.5683 - val_accuracy: 0.2208\n",
      "Epoch 167/700\n",
      "1700/1700 [==============================] - 1s 636us/step - loss: 0.8727 - accuracy: 0.6882 - val_loss: 3.4721 - val_accuracy: 0.2438\n",
      "Epoch 168/700\n",
      "1700/1700 [==============================] - 1s 685us/step - loss: 0.8782 - accuracy: 0.6824 - val_loss: 3.7592 - val_accuracy: 0.2188\n",
      "Epoch 169/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8667 - accuracy: 0.6953 - val_loss: 3.3196 - val_accuracy: 0.2250\n",
      "Epoch 170/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.8645 - accuracy: 0.6994 - val_loss: 3.4218 - val_accuracy: 0.2375\n",
      "Epoch 171/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.8779 - accuracy: 0.6965 - val_loss: 3.6562 - val_accuracy: 0.2229\n",
      "Epoch 172/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.8593 - accuracy: 0.6935 - val_loss: 3.7887 - val_accuracy: 0.2271\n",
      "Epoch 173/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.8683 - accuracy: 0.6959 - val_loss: 3.9373 - val_accuracy: 0.2188\n",
      "Epoch 174/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.8570 - accuracy: 0.6988 - val_loss: 3.9266 - val_accuracy: 0.2479\n",
      "Epoch 175/700\n",
      "1700/1700 [==============================] - 1s 604us/step - loss: 0.8599 - accuracy: 0.6988 - val_loss: 3.7153 - val_accuracy: 0.2313\n",
      "Epoch 176/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.8557 - accuracy: 0.7000 - val_loss: 3.5240 - val_accuracy: 0.2375\n",
      "Epoch 177/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.8566 - accuracy: 0.6894 - val_loss: 3.6701 - val_accuracy: 0.2375\n",
      "Epoch 178/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.8476 - accuracy: 0.6959 - val_loss: 3.9225 - val_accuracy: 0.2417\n",
      "Epoch 179/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.8476 - accuracy: 0.6971 - val_loss: 3.7033 - val_accuracy: 0.2292\n",
      "Epoch 180/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.8476 - accuracy: 0.6971 - val_loss: 3.4555 - val_accuracy: 0.2313\n",
      "Epoch 181/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.8482 - accuracy: 0.6994 - val_loss: 3.8430 - val_accuracy: 0.2375\n",
      "Epoch 182/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.8352 - accuracy: 0.7053 - val_loss: 3.6831 - val_accuracy: 0.2396\n",
      "Epoch 183/700\n",
      "1700/1700 [==============================] - 1s 638us/step - loss: 0.8432 - accuracy: 0.7012 - val_loss: 4.0140 - val_accuracy: 0.2250\n",
      "Epoch 184/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.8384 - accuracy: 0.7006 - val_loss: 3.5380 - val_accuracy: 0.2167\n",
      "Epoch 185/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8416 - accuracy: 0.7118 - val_loss: 3.8730 - val_accuracy: 0.2208\n",
      "Epoch 186/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.8351 - accuracy: 0.6959 - val_loss: 3.8041 - val_accuracy: 0.2396\n",
      "Epoch 187/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8380 - accuracy: 0.7076 - val_loss: 3.6138 - val_accuracy: 0.2250\n",
      "Epoch 188/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8272 - accuracy: 0.7035 - val_loss: 3.7396 - val_accuracy: 0.2375\n",
      "Epoch 189/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.8243 - accuracy: 0.7141 - val_loss: 3.4800 - val_accuracy: 0.2250\n",
      "Epoch 190/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.8233 - accuracy: 0.7159 - val_loss: 3.9750 - val_accuracy: 0.2375\n",
      "Epoch 191/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.8244 - accuracy: 0.7047 - val_loss: 3.6993 - val_accuracy: 0.2313\n",
      "Epoch 192/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.8248 - accuracy: 0.7094 - val_loss: 3.6537 - val_accuracy: 0.2313\n",
      "Epoch 193/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.8193 - accuracy: 0.7194 - val_loss: 3.7564 - val_accuracy: 0.2188\n",
      "Epoch 194/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.8192 - accuracy: 0.7153 - val_loss: 4.0673 - val_accuracy: 0.2375\n",
      "Epoch 195/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.8060 - accuracy: 0.7212 - val_loss: 3.8735 - val_accuracy: 0.2313\n",
      "Epoch 196/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.8091 - accuracy: 0.7200 - val_loss: 3.7857 - val_accuracy: 0.2396\n",
      "Epoch 197/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.8119 - accuracy: 0.7041 - val_loss: 3.7605 - val_accuracy: 0.2250\n",
      "Epoch 198/700\n",
      "1700/1700 [==============================] - 1s 616us/step - loss: 0.8168 - accuracy: 0.7135 - val_loss: 4.3302 - val_accuracy: 0.2313\n",
      "Epoch 199/700\n",
      "1700/1700 [==============================] - 1s 649us/step - loss: 0.8019 - accuracy: 0.7118 - val_loss: 3.8273 - val_accuracy: 0.2250\n",
      "Epoch 200/700\n",
      "1700/1700 [==============================] - 1s 603us/step - loss: 0.8021 - accuracy: 0.7188 - val_loss: 3.7631 - val_accuracy: 0.2313\n",
      "Epoch 201/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.7984 - accuracy: 0.7282 - val_loss: 4.0482 - val_accuracy: 0.2396\n",
      "Epoch 202/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.8011 - accuracy: 0.7165 - val_loss: 4.0303 - val_accuracy: 0.2396\n",
      "Epoch 203/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.8009 - accuracy: 0.7088 - val_loss: 4.2791 - val_accuracy: 0.2333\n",
      "Epoch 204/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 0.7992 - accuracy: 0.7188 - val_loss: 4.1463 - val_accuracy: 0.2271\n",
      "Epoch 205/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 0.8052 - accuracy: 0.7124 - val_loss: 3.9700 - val_accuracy: 0.2250\n",
      "Epoch 206/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 0.7850 - accuracy: 0.7318 - val_loss: 4.0367 - val_accuracy: 0.2375\n",
      "Epoch 207/700\n",
      "1700/1700 [==============================] - 1s 648us/step - loss: 0.7942 - accuracy: 0.7200 - val_loss: 4.2141 - val_accuracy: 0.2396\n",
      "Epoch 208/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 0.7895 - accuracy: 0.7247 - val_loss: 3.7893 - val_accuracy: 0.2250\n",
      "Epoch 209/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7906 - accuracy: 0.7282 - val_loss: 3.8107 - val_accuracy: 0.2375\n",
      "Epoch 210/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7884 - accuracy: 0.7265 - val_loss: 4.0573 - val_accuracy: 0.2375\n",
      "Epoch 211/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.7872 - accuracy: 0.7241 - val_loss: 3.7218 - val_accuracy: 0.2313\n",
      "Epoch 212/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.7865 - accuracy: 0.7200 - val_loss: 4.2290 - val_accuracy: 0.2250\n",
      "Epoch 213/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.7768 - accuracy: 0.7324 - val_loss: 3.9129 - val_accuracy: 0.2292\n",
      "Epoch 214/700\n",
      "1700/1700 [==============================] - 1s 642us/step - loss: 0.7813 - accuracy: 0.7329 - val_loss: 4.0309 - val_accuracy: 0.2375\n",
      "Epoch 215/700\n",
      "1700/1700 [==============================] - 1s 618us/step - loss: 0.7830 - accuracy: 0.7253 - val_loss: 4.0623 - val_accuracy: 0.2313\n",
      "Epoch 216/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.7757 - accuracy: 0.7218 - val_loss: 4.0425 - val_accuracy: 0.2396\n",
      "Epoch 217/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.7677 - accuracy: 0.7288 - val_loss: 4.0977 - val_accuracy: 0.2375\n",
      "Epoch 218/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7738 - accuracy: 0.7241 - val_loss: 4.0780 - val_accuracy: 0.2271\n",
      "Epoch 219/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.7624 - accuracy: 0.7382 - val_loss: 4.3928 - val_accuracy: 0.2417\n",
      "Epoch 220/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.7692 - accuracy: 0.7324 - val_loss: 4.0922 - val_accuracy: 0.2354\n",
      "Epoch 221/700\n",
      "1700/1700 [==============================] - 1s 586us/step - loss: 0.7671 - accuracy: 0.7347 - val_loss: 4.3305 - val_accuracy: 0.2271\n",
      "Epoch 222/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.7642 - accuracy: 0.7312 - val_loss: 4.2320 - val_accuracy: 0.2354\n",
      "Epoch 223/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.7676 - accuracy: 0.7324 - val_loss: 4.2561 - val_accuracy: 0.2188\n",
      "Epoch 224/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.7596 - accuracy: 0.7359 - val_loss: 4.3105 - val_accuracy: 0.2292\n",
      "Epoch 225/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.7582 - accuracy: 0.7400 - val_loss: 4.0382 - val_accuracy: 0.2271\n",
      "Epoch 226/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.7625 - accuracy: 0.7347 - val_loss: 4.2378 - val_accuracy: 0.2292\n",
      "Epoch 227/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7582 - accuracy: 0.7194 - val_loss: 4.2238 - val_accuracy: 0.2208\n",
      "Epoch 228/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.7496 - accuracy: 0.7394 - val_loss: 4.1667 - val_accuracy: 0.2354\n",
      "Epoch 229/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.7480 - accuracy: 0.7335 - val_loss: 4.1768 - val_accuracy: 0.2292\n",
      "Epoch 230/700\n",
      "1700/1700 [==============================] - 1s 635us/step - loss: 0.7437 - accuracy: 0.7476 - val_loss: 3.8958 - val_accuracy: 0.2333\n",
      "Epoch 231/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.7413 - accuracy: 0.7518 - val_loss: 4.0306 - val_accuracy: 0.2313\n",
      "Epoch 232/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.7444 - accuracy: 0.7429 - val_loss: 4.3501 - val_accuracy: 0.2333\n",
      "Epoch 233/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.7488 - accuracy: 0.7400 - val_loss: 4.1462 - val_accuracy: 0.2250\n",
      "Epoch 234/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.7476 - accuracy: 0.7394 - val_loss: 4.1077 - val_accuracy: 0.2313\n",
      "Epoch 235/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.7454 - accuracy: 0.7306 - val_loss: 4.3680 - val_accuracy: 0.2333\n",
      "Epoch 236/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.7392 - accuracy: 0.7453 - val_loss: 4.3590 - val_accuracy: 0.2271\n",
      "Epoch 237/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.7424 - accuracy: 0.7412 - val_loss: 4.5354 - val_accuracy: 0.2396\n",
      "Epoch 238/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.7319 - accuracy: 0.7529 - val_loss: 4.2467 - val_accuracy: 0.2292\n",
      "Epoch 239/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.7284 - accuracy: 0.7453 - val_loss: 4.0748 - val_accuracy: 0.2292\n",
      "Epoch 240/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7214 - accuracy: 0.7512 - val_loss: 4.5940 - val_accuracy: 0.2375\n",
      "Epoch 241/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.7320 - accuracy: 0.7394 - val_loss: 4.5118 - val_accuracy: 0.2354\n",
      "Epoch 242/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.7358 - accuracy: 0.7412 - val_loss: 4.4876 - val_accuracy: 0.2208\n",
      "Epoch 243/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.7179 - accuracy: 0.7629 - val_loss: 4.5345 - val_accuracy: 0.2313\n",
      "Epoch 244/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.7226 - accuracy: 0.7453 - val_loss: 4.2455 - val_accuracy: 0.2250\n",
      "Epoch 245/700\n",
      "1700/1700 [==============================] - 1s 608us/step - loss: 0.7219 - accuracy: 0.7500 - val_loss: 4.5615 - val_accuracy: 0.2354\n",
      "Epoch 246/700\n",
      "1700/1700 [==============================] - 1s 636us/step - loss: 0.7292 - accuracy: 0.7324 - val_loss: 4.4142 - val_accuracy: 0.2375\n",
      "Epoch 247/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.7263 - accuracy: 0.7412 - val_loss: 4.4218 - val_accuracy: 0.2396\n",
      "Epoch 248/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7132 - accuracy: 0.7582 - val_loss: 4.4524 - val_accuracy: 0.2396\n",
      "Epoch 249/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7218 - accuracy: 0.7482 - val_loss: 4.3149 - val_accuracy: 0.2229\n",
      "Epoch 250/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7127 - accuracy: 0.7482 - val_loss: 4.5139 - val_accuracy: 0.2271\n",
      "Epoch 251/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.7138 - accuracy: 0.7524 - val_loss: 4.6645 - val_accuracy: 0.2208\n",
      "Epoch 252/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.7085 - accuracy: 0.7506 - val_loss: 4.3991 - val_accuracy: 0.2354\n",
      "Epoch 253/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.7065 - accuracy: 0.7624 - val_loss: 4.3324 - val_accuracy: 0.2354\n",
      "Epoch 254/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.6991 - accuracy: 0.7559 - val_loss: 4.4717 - val_accuracy: 0.2354\n",
      "Epoch 255/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.7090 - accuracy: 0.7582 - val_loss: 4.4898 - val_accuracy: 0.2313\n",
      "Epoch 256/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.7046 - accuracy: 0.7476 - val_loss: 4.3805 - val_accuracy: 0.2313\n",
      "Epoch 257/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.7018 - accuracy: 0.7435 - val_loss: 4.4419 - val_accuracy: 0.2375\n",
      "Epoch 258/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.6900 - accuracy: 0.7612 - val_loss: 4.4828 - val_accuracy: 0.2333\n",
      "Epoch 259/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.7022 - accuracy: 0.7665 - val_loss: 4.4459 - val_accuracy: 0.2354\n",
      "Epoch 260/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.6956 - accuracy: 0.7576 - val_loss: 4.6756 - val_accuracy: 0.2396\n",
      "Epoch 261/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.6850 - accuracy: 0.7612 - val_loss: 4.6294 - val_accuracy: 0.2250\n",
      "Epoch 262/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.6907 - accuracy: 0.7629 - val_loss: 4.6159 - val_accuracy: 0.2250\n",
      "Epoch 263/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.6830 - accuracy: 0.7647 - val_loss: 4.3719 - val_accuracy: 0.2313\n",
      "Epoch 264/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.6820 - accuracy: 0.7594 - val_loss: 4.7010 - val_accuracy: 0.2292\n",
      "Epoch 265/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.6790 - accuracy: 0.7653 - val_loss: 4.4510 - val_accuracy: 0.2313\n",
      "Epoch 266/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.6924 - accuracy: 0.7576 - val_loss: 4.6164 - val_accuracy: 0.2292\n",
      "Epoch 267/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.6807 - accuracy: 0.7747 - val_loss: 4.3092 - val_accuracy: 0.2313\n",
      "Epoch 268/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.6800 - accuracy: 0.7653 - val_loss: 4.4039 - val_accuracy: 0.2354\n",
      "Epoch 269/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.6761 - accuracy: 0.7653 - val_loss: 4.4418 - val_accuracy: 0.2313\n",
      "Epoch 270/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.6811 - accuracy: 0.7653 - val_loss: 4.6541 - val_accuracy: 0.2271\n",
      "Epoch 271/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.6746 - accuracy: 0.7676 - val_loss: 4.5073 - val_accuracy: 0.2354\n",
      "Epoch 272/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.6727 - accuracy: 0.7624 - val_loss: 4.6047 - val_accuracy: 0.2313\n",
      "Epoch 273/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.6753 - accuracy: 0.7600 - val_loss: 4.2610 - val_accuracy: 0.2333\n",
      "Epoch 274/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.6789 - accuracy: 0.7688 - val_loss: 4.3388 - val_accuracy: 0.2313\n",
      "Epoch 275/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.6696 - accuracy: 0.7735 - val_loss: 4.4063 - val_accuracy: 0.2313\n",
      "Epoch 276/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.6574 - accuracy: 0.7765 - val_loss: 4.6246 - val_accuracy: 0.2354\n",
      "Epoch 277/700\n",
      "1700/1700 [==============================] - 1s 630us/step - loss: 0.6685 - accuracy: 0.7688 - val_loss: 4.5689 - val_accuracy: 0.2333\n",
      "Epoch 278/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 0.6611 - accuracy: 0.7724 - val_loss: 4.5100 - val_accuracy: 0.2313\n",
      "Epoch 279/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.6612 - accuracy: 0.7694 - val_loss: 4.5031 - val_accuracy: 0.2313\n",
      "Epoch 280/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.6612 - accuracy: 0.7706 - val_loss: 4.5996 - val_accuracy: 0.2313\n",
      "Epoch 281/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.6668 - accuracy: 0.7729 - val_loss: 4.7168 - val_accuracy: 0.2354\n",
      "Epoch 282/700\n",
      "1700/1700 [==============================] - 1s 634us/step - loss: 0.6609 - accuracy: 0.7776 - val_loss: 4.6762 - val_accuracy: 0.2250\n",
      "Epoch 283/700\n",
      "1700/1700 [==============================] - 1s 673us/step - loss: 0.6568 - accuracy: 0.7659 - val_loss: 4.4481 - val_accuracy: 0.2313\n",
      "Epoch 284/700\n",
      "1700/1700 [==============================] - 1s 666us/step - loss: 0.6557 - accuracy: 0.7800 - val_loss: 4.5751 - val_accuracy: 0.2292\n",
      "Epoch 285/700\n",
      "1700/1700 [==============================] - 1s 631us/step - loss: 0.6562 - accuracy: 0.7771 - val_loss: 4.6821 - val_accuracy: 0.2292\n",
      "Epoch 286/700\n",
      "1700/1700 [==============================] - 1s 633us/step - loss: 0.6602 - accuracy: 0.7794 - val_loss: 5.1120 - val_accuracy: 0.2333\n",
      "Epoch 287/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.6318 - accuracy: 0.7847 - val_loss: 4.5688 - val_accuracy: 0.2292\n",
      "Epoch 288/700\n",
      "1700/1700 [==============================] - 1s 629us/step - loss: 0.6408 - accuracy: 0.7782 - val_loss: 4.5941 - val_accuracy: 0.2354\n",
      "Epoch 289/700\n",
      "1700/1700 [==============================] - 1s 624us/step - loss: 0.6424 - accuracy: 0.7824 - val_loss: 4.6895 - val_accuracy: 0.2313\n",
      "Epoch 290/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.6488 - accuracy: 0.7729 - val_loss: 4.5024 - val_accuracy: 0.2313\n",
      "Epoch 291/700\n",
      "1700/1700 [==============================] - 1s 610us/step - loss: 0.6407 - accuracy: 0.7706 - val_loss: 4.8956 - val_accuracy: 0.2313\n",
      "Epoch 292/700\n",
      "1700/1700 [==============================] - 1s 655us/step - loss: 0.6473 - accuracy: 0.7818 - val_loss: 4.4536 - val_accuracy: 0.2354\n",
      "Epoch 293/700\n",
      "1700/1700 [==============================] - 1s 622us/step - loss: 0.6520 - accuracy: 0.7800 - val_loss: 4.6625 - val_accuracy: 0.2375\n",
      "Epoch 294/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.6451 - accuracy: 0.7735 - val_loss: 4.6673 - val_accuracy: 0.2313\n",
      "Epoch 295/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.6311 - accuracy: 0.7806 - val_loss: 4.5926 - val_accuracy: 0.2292\n",
      "Epoch 296/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.6305 - accuracy: 0.7841 - val_loss: 4.3804 - val_accuracy: 0.2292\n",
      "Epoch 297/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.6351 - accuracy: 0.7829 - val_loss: 4.7574 - val_accuracy: 0.2333\n",
      "Epoch 298/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.6402 - accuracy: 0.7753 - val_loss: 5.2190 - val_accuracy: 0.2313\n",
      "Epoch 299/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.6351 - accuracy: 0.7918 - val_loss: 4.8304 - val_accuracy: 0.2313\n",
      "Epoch 300/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.6298 - accuracy: 0.7906 - val_loss: 4.5690 - val_accuracy: 0.2313\n",
      "Epoch 301/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.6435 - accuracy: 0.7847 - val_loss: 5.0657 - val_accuracy: 0.2396\n",
      "Epoch 302/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.6331 - accuracy: 0.7847 - val_loss: 4.5544 - val_accuracy: 0.2313\n",
      "Epoch 303/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.6240 - accuracy: 0.7788 - val_loss: 4.8468 - val_accuracy: 0.2313\n",
      "Epoch 304/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.6324 - accuracy: 0.7841 - val_loss: 4.7994 - val_accuracy: 0.2333\n",
      "Epoch 305/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.6251 - accuracy: 0.7865 - val_loss: 4.9184 - val_accuracy: 0.2313\n",
      "Epoch 306/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.6240 - accuracy: 0.7871 - val_loss: 5.1546 - val_accuracy: 0.2313\n",
      "Epoch 307/700\n",
      "1700/1700 [==============================] - 1s 604us/step - loss: 0.6244 - accuracy: 0.7806 - val_loss: 4.7845 - val_accuracy: 0.2333\n",
      "Epoch 308/700\n",
      "1700/1700 [==============================] - 1s 639us/step - loss: 0.6204 - accuracy: 0.7876 - val_loss: 4.6871 - val_accuracy: 0.2313\n",
      "Epoch 309/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.6227 - accuracy: 0.7824 - val_loss: 5.1654 - val_accuracy: 0.2375\n",
      "Epoch 310/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.6079 - accuracy: 0.8012 - val_loss: 4.7050 - val_accuracy: 0.2333\n",
      "Epoch 311/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.6091 - accuracy: 0.7953 - val_loss: 4.9540 - val_accuracy: 0.2250\n",
      "Epoch 312/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.6260 - accuracy: 0.7959 - val_loss: 4.8767 - val_accuracy: 0.2333\n",
      "Epoch 313/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.6006 - accuracy: 0.7888 - val_loss: 4.6795 - val_accuracy: 0.2313\n",
      "Epoch 314/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.6121 - accuracy: 0.7859 - val_loss: 5.1493 - val_accuracy: 0.2333\n",
      "Epoch 315/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.6099 - accuracy: 0.7818 - val_loss: 4.6946 - val_accuracy: 0.2313\n",
      "Epoch 316/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.6118 - accuracy: 0.7859 - val_loss: 4.8354 - val_accuracy: 0.2354\n",
      "Epoch 317/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.6013 - accuracy: 0.7935 - val_loss: 4.5950 - val_accuracy: 0.2375\n",
      "Epoch 318/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.6001 - accuracy: 0.7888 - val_loss: 5.3327 - val_accuracy: 0.2313\n",
      "Epoch 319/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.6162 - accuracy: 0.7906 - val_loss: 5.0125 - val_accuracy: 0.2313\n",
      "Epoch 320/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.5904 - accuracy: 0.7935 - val_loss: 4.7491 - val_accuracy: 0.2333\n",
      "Epoch 321/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.5999 - accuracy: 0.7959 - val_loss: 5.0922 - val_accuracy: 0.2333\n",
      "Epoch 322/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.5969 - accuracy: 0.7947 - val_loss: 5.0060 - val_accuracy: 0.2354\n",
      "Epoch 323/700\n",
      "1700/1700 [==============================] - 1s 616us/step - loss: 0.5995 - accuracy: 0.7912 - val_loss: 5.2442 - val_accuracy: 0.2292\n",
      "Epoch 324/700\n",
      "1700/1700 [==============================] - 1s 632us/step - loss: 0.5985 - accuracy: 0.8059 - val_loss: 5.2996 - val_accuracy: 0.2292\n",
      "Epoch 325/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.5923 - accuracy: 0.7965 - val_loss: 4.8323 - val_accuracy: 0.2375\n",
      "Epoch 326/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.5863 - accuracy: 0.8065 - val_loss: 4.7314 - val_accuracy: 0.2292\n",
      "Epoch 327/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.5871 - accuracy: 0.8088 - val_loss: 5.3455 - val_accuracy: 0.2313\n",
      "Epoch 328/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5984 - accuracy: 0.7976 - val_loss: 5.6877 - val_accuracy: 0.2313\n",
      "Epoch 329/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.5858 - accuracy: 0.7988 - val_loss: 4.9975 - val_accuracy: 0.2292\n",
      "Epoch 330/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.5818 - accuracy: 0.8106 - val_loss: 5.0370 - val_accuracy: 0.2313\n",
      "Epoch 331/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.5763 - accuracy: 0.7988 - val_loss: 5.0675 - val_accuracy: 0.2313\n",
      "Epoch 332/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5764 - accuracy: 0.8000 - val_loss: 5.0392 - val_accuracy: 0.2333\n",
      "Epoch 333/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.5721 - accuracy: 0.8135 - val_loss: 5.0761 - val_accuracy: 0.2292\n",
      "Epoch 334/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5804 - accuracy: 0.7959 - val_loss: 5.3012 - val_accuracy: 0.2292\n",
      "Epoch 335/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5799 - accuracy: 0.8065 - val_loss: 5.0783 - val_accuracy: 0.2292\n",
      "Epoch 336/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.5873 - accuracy: 0.8006 - val_loss: 4.9540 - val_accuracy: 0.2250\n",
      "Epoch 337/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.5705 - accuracy: 0.8047 - val_loss: 5.0840 - val_accuracy: 0.2333\n",
      "Epoch 338/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.5737 - accuracy: 0.8071 - val_loss: 4.9959 - val_accuracy: 0.2333\n",
      "Epoch 339/700\n",
      "1700/1700 [==============================] - 1s 620us/step - loss: 0.5715 - accuracy: 0.8106 - val_loss: 4.9715 - val_accuracy: 0.2333\n",
      "Epoch 340/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 0.5681 - accuracy: 0.8053 - val_loss: 5.1945 - val_accuracy: 0.2313\n",
      "Epoch 341/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.5776 - accuracy: 0.8129 - val_loss: 4.9642 - val_accuracy: 0.2333\n",
      "Epoch 342/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.5671 - accuracy: 0.8100 - val_loss: 5.3357 - val_accuracy: 0.2333\n",
      "Epoch 343/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.5587 - accuracy: 0.8000 - val_loss: 5.1262 - val_accuracy: 0.2333\n",
      "Epoch 344/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5593 - accuracy: 0.8159 - val_loss: 5.2315 - val_accuracy: 0.2333\n",
      "Epoch 345/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.5584 - accuracy: 0.8000 - val_loss: 5.0506 - val_accuracy: 0.2313\n",
      "Epoch 346/700\n",
      "1700/1700 [==============================] - 1s 620us/step - loss: 0.5573 - accuracy: 0.8165 - val_loss: 4.9484 - val_accuracy: 0.2313\n",
      "Epoch 347/700\n",
      "1700/1700 [==============================] - 1s 628us/step - loss: 0.5688 - accuracy: 0.8094 - val_loss: 5.0394 - val_accuracy: 0.2313\n",
      "Epoch 348/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.5770 - accuracy: 0.8047 - val_loss: 5.0262 - val_accuracy: 0.2333\n",
      "Epoch 349/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.5636 - accuracy: 0.8141 - val_loss: 4.9953 - val_accuracy: 0.2292\n",
      "Epoch 350/700\n",
      "1700/1700 [==============================] - 1s 626us/step - loss: 0.5490 - accuracy: 0.8159 - val_loss: 5.2076 - val_accuracy: 0.2292\n",
      "Epoch 351/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 0.5450 - accuracy: 0.8176 - val_loss: 4.8538 - val_accuracy: 0.2333\n",
      "Epoch 352/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.5604 - accuracy: 0.7947 - val_loss: 4.9448 - val_accuracy: 0.2333\n",
      "Epoch 353/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.5509 - accuracy: 0.8088 - val_loss: 5.0657 - val_accuracy: 0.2313\n",
      "Epoch 354/700\n",
      "1700/1700 [==============================] - 1s 607us/step - loss: 0.5609 - accuracy: 0.8088 - val_loss: 5.0150 - val_accuracy: 0.2292\n",
      "Epoch 355/700\n",
      "1700/1700 [==============================] - 1s 637us/step - loss: 0.5529 - accuracy: 0.8129 - val_loss: 5.1724 - val_accuracy: 0.2354\n",
      "Epoch 356/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.5493 - accuracy: 0.8153 - val_loss: 5.3235 - val_accuracy: 0.2333\n",
      "Epoch 357/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.5555 - accuracy: 0.8041 - val_loss: 5.2993 - val_accuracy: 0.2313\n",
      "Epoch 358/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.5506 - accuracy: 0.8165 - val_loss: 5.4856 - val_accuracy: 0.2292\n",
      "Epoch 359/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.5418 - accuracy: 0.8265 - val_loss: 4.7997 - val_accuracy: 0.2333\n",
      "Epoch 360/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5362 - accuracy: 0.8171 - val_loss: 5.0484 - val_accuracy: 0.2292\n",
      "Epoch 361/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.5457 - accuracy: 0.8165 - val_loss: 5.5064 - val_accuracy: 0.2292\n",
      "Epoch 362/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5378 - accuracy: 0.8135 - val_loss: 5.4323 - val_accuracy: 0.2292\n",
      "Epoch 363/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.5411 - accuracy: 0.8135 - val_loss: 5.8142 - val_accuracy: 0.2313\n",
      "Epoch 364/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.5437 - accuracy: 0.8182 - val_loss: 5.0418 - val_accuracy: 0.2333\n",
      "Epoch 365/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.5343 - accuracy: 0.8159 - val_loss: 5.2013 - val_accuracy: 0.2333\n",
      "Epoch 366/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5406 - accuracy: 0.8253 - val_loss: 5.1862 - val_accuracy: 0.2333\n",
      "Epoch 367/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.5329 - accuracy: 0.8135 - val_loss: 5.4921 - val_accuracy: 0.2354\n",
      "Epoch 368/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5308 - accuracy: 0.8171 - val_loss: 5.1360 - val_accuracy: 0.2271\n",
      "Epoch 369/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.5379 - accuracy: 0.8176 - val_loss: 5.1879 - val_accuracy: 0.2313\n",
      "Epoch 370/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 0.5319 - accuracy: 0.8218 - val_loss: 5.1035 - val_accuracy: 0.2375\n",
      "Epoch 371/700\n",
      "1700/1700 [==============================] - 1s 637us/step - loss: 0.5299 - accuracy: 0.8171 - val_loss: 5.2628 - val_accuracy: 0.2292\n",
      "Epoch 372/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.5251 - accuracy: 0.8247 - val_loss: 5.1370 - val_accuracy: 0.2396\n",
      "Epoch 373/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.5292 - accuracy: 0.8153 - val_loss: 5.4364 - val_accuracy: 0.2292\n",
      "Epoch 374/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.5249 - accuracy: 0.8253 - val_loss: 5.6266 - val_accuracy: 0.2333\n",
      "Epoch 375/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.5243 - accuracy: 0.8141 - val_loss: 5.6579 - val_accuracy: 0.2313\n",
      "Epoch 376/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.5197 - accuracy: 0.8212 - val_loss: 5.6530 - val_accuracy: 0.2292\n",
      "Epoch 377/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5161 - accuracy: 0.8241 - val_loss: 5.7735 - val_accuracy: 0.2313\n",
      "Epoch 378/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.5206 - accuracy: 0.8329 - val_loss: 5.6088 - val_accuracy: 0.2313\n",
      "Epoch 379/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.5193 - accuracy: 0.8259 - val_loss: 5.5765 - val_accuracy: 0.2313\n",
      "Epoch 380/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.5278 - accuracy: 0.8253 - val_loss: 5.3313 - val_accuracy: 0.2417\n",
      "Epoch 381/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.5154 - accuracy: 0.8224 - val_loss: 5.3913 - val_accuracy: 0.2292\n",
      "Epoch 382/700\n",
      "1700/1700 [==============================] - 1s 602us/step - loss: 0.5136 - accuracy: 0.8218 - val_loss: 5.8478 - val_accuracy: 0.2333\n",
      "Epoch 383/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.5106 - accuracy: 0.8253 - val_loss: 5.1648 - val_accuracy: 0.2333\n",
      "Epoch 384/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.5105 - accuracy: 0.8341 - val_loss: 5.7994 - val_accuracy: 0.2271\n",
      "Epoch 385/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.5156 - accuracy: 0.8271 - val_loss: 5.3957 - val_accuracy: 0.2333\n",
      "Epoch 386/700\n",
      "1700/1700 [==============================] - 1s 619us/step - loss: 0.5023 - accuracy: 0.8276 - val_loss: 5.7033 - val_accuracy: 0.2313\n",
      "Epoch 387/700\n",
      "1700/1700 [==============================] - 1s 623us/step - loss: 0.4993 - accuracy: 0.8376 - val_loss: 5.6709 - val_accuracy: 0.2313\n",
      "Epoch 388/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.5059 - accuracy: 0.8324 - val_loss: 5.8584 - val_accuracy: 0.2271\n",
      "Epoch 389/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.5119 - accuracy: 0.8271 - val_loss: 5.3601 - val_accuracy: 0.2313\n",
      "Epoch 390/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.5094 - accuracy: 0.8300 - val_loss: 5.6379 - val_accuracy: 0.2333\n",
      "Epoch 391/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.5028 - accuracy: 0.8347 - val_loss: 5.2770 - val_accuracy: 0.2271\n",
      "Epoch 392/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.5000 - accuracy: 0.8329 - val_loss: 5.4871 - val_accuracy: 0.2333\n",
      "Epoch 393/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.5000 - accuracy: 0.8335 - val_loss: 5.9588 - val_accuracy: 0.2354\n",
      "Epoch 394/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.5085 - accuracy: 0.8300 - val_loss: 6.0615 - val_accuracy: 0.2396\n",
      "Epoch 395/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.5070 - accuracy: 0.8306 - val_loss: 5.2460 - val_accuracy: 0.2333\n",
      "Epoch 396/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.5001 - accuracy: 0.8294 - val_loss: 5.8785 - val_accuracy: 0.2313\n",
      "Epoch 397/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.4897 - accuracy: 0.8418 - val_loss: 5.8052 - val_accuracy: 0.2333\n",
      "Epoch 398/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.4944 - accuracy: 0.8312 - val_loss: 5.7172 - val_accuracy: 0.2333\n",
      "Epoch 399/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.4897 - accuracy: 0.8300 - val_loss: 5.7525 - val_accuracy: 0.2292\n",
      "Epoch 400/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.4904 - accuracy: 0.8300 - val_loss: 5.7097 - val_accuracy: 0.2250\n",
      "Epoch 401/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.5001 - accuracy: 0.8282 - val_loss: 5.5356 - val_accuracy: 0.2292\n",
      "Epoch 402/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.4908 - accuracy: 0.8388 - val_loss: 5.9020 - val_accuracy: 0.2313\n",
      "Epoch 403/700\n",
      "1700/1700 [==============================] - 1s 614us/step - loss: 0.4870 - accuracy: 0.8394 - val_loss: 5.9103 - val_accuracy: 0.2292\n",
      "Epoch 404/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.4882 - accuracy: 0.8324 - val_loss: 5.9343 - val_accuracy: 0.2375\n",
      "Epoch 405/700\n",
      "1700/1700 [==============================] - 1s 608us/step - loss: 0.4779 - accuracy: 0.8376 - val_loss: 5.6420 - val_accuracy: 0.2292\n",
      "Epoch 406/700\n",
      "1700/1700 [==============================] - 1s 624us/step - loss: 0.4945 - accuracy: 0.8435 - val_loss: 5.7889 - val_accuracy: 0.2313\n",
      "Epoch 407/700\n",
      "1700/1700 [==============================] - 1s 626us/step - loss: 0.4759 - accuracy: 0.8371 - val_loss: 5.7912 - val_accuracy: 0.2333\n",
      "Epoch 408/700\n",
      "1700/1700 [==============================] - 1s 628us/step - loss: 0.4868 - accuracy: 0.8371 - val_loss: 5.5613 - val_accuracy: 0.2333\n",
      "Epoch 409/700\n",
      "1700/1700 [==============================] - 1s 623us/step - loss: 0.4822 - accuracy: 0.8406 - val_loss: 5.6494 - val_accuracy: 0.2313\n",
      "Epoch 410/700\n",
      "1700/1700 [==============================] - 1s 623us/step - loss: 0.4677 - accuracy: 0.8488 - val_loss: 5.8447 - val_accuracy: 0.2271\n",
      "Epoch 411/700\n",
      "1700/1700 [==============================] - 1s 616us/step - loss: 0.4778 - accuracy: 0.8359 - val_loss: 5.6463 - val_accuracy: 0.2333\n",
      "Epoch 412/700\n",
      "1700/1700 [==============================] - 1s 608us/step - loss: 0.4719 - accuracy: 0.8453 - val_loss: 5.7115 - val_accuracy: 0.2250\n",
      "Epoch 413/700\n",
      "1700/1700 [==============================] - 1s 607us/step - loss: 0.4769 - accuracy: 0.8471 - val_loss: 5.6021 - val_accuracy: 0.2292\n",
      "Epoch 414/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.4724 - accuracy: 0.8382 - val_loss: 5.7535 - val_accuracy: 0.2333\n",
      "Epoch 415/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.4749 - accuracy: 0.8365 - val_loss: 5.9734 - val_accuracy: 0.2292\n",
      "Epoch 416/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.4787 - accuracy: 0.8494 - val_loss: 5.6062 - val_accuracy: 0.2313\n",
      "Epoch 417/700\n",
      "1700/1700 [==============================] - 1s 605us/step - loss: 0.4727 - accuracy: 0.8394 - val_loss: 5.9018 - val_accuracy: 0.2333\n",
      "Epoch 418/700\n",
      "1700/1700 [==============================] - 1s 635us/step - loss: 0.4613 - accuracy: 0.8488 - val_loss: 5.9813 - val_accuracy: 0.2333\n",
      "Epoch 419/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.4673 - accuracy: 0.8388 - val_loss: 5.7981 - val_accuracy: 0.2333\n",
      "Epoch 420/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.4687 - accuracy: 0.8459 - val_loss: 5.6956 - val_accuracy: 0.2271\n",
      "Epoch 421/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.4611 - accuracy: 0.8476 - val_loss: 5.9227 - val_accuracy: 0.2250\n",
      "Epoch 422/700\n",
      "1700/1700 [==============================] - 1s 586us/step - loss: 0.4562 - accuracy: 0.8524 - val_loss: 5.8081 - val_accuracy: 0.2313\n",
      "Epoch 423/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.4627 - accuracy: 0.8441 - val_loss: 6.1996 - val_accuracy: 0.2333\n",
      "Epoch 424/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.4561 - accuracy: 0.8500 - val_loss: 5.6792 - val_accuracy: 0.2333\n",
      "Epoch 425/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.4641 - accuracy: 0.8453 - val_loss: 5.7770 - val_accuracy: 0.2375\n",
      "Epoch 426/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.4622 - accuracy: 0.8459 - val_loss: 5.7935 - val_accuracy: 0.2229\n",
      "Epoch 427/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.4517 - accuracy: 0.8624 - val_loss: 6.3317 - val_accuracy: 0.2396\n",
      "Epoch 428/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.4575 - accuracy: 0.8482 - val_loss: 6.0268 - val_accuracy: 0.2229\n",
      "Epoch 429/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.4640 - accuracy: 0.8447 - val_loss: 6.0456 - val_accuracy: 0.2354\n",
      "Epoch 430/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.4511 - accuracy: 0.8506 - val_loss: 5.7350 - val_accuracy: 0.2292\n",
      "Epoch 431/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.4452 - accuracy: 0.8565 - val_loss: 5.2760 - val_accuracy: 0.2250\n",
      "Epoch 432/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.4603 - accuracy: 0.8382 - val_loss: 6.2194 - val_accuracy: 0.2333\n",
      "Epoch 433/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 0.4552 - accuracy: 0.8482 - val_loss: 5.9638 - val_accuracy: 0.2292\n",
      "Epoch 434/700\n",
      "1700/1700 [==============================] - 1s 633us/step - loss: 0.4550 - accuracy: 0.8429 - val_loss: 5.7305 - val_accuracy: 0.2354\n",
      "Epoch 435/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.4422 - accuracy: 0.8576 - val_loss: 5.9664 - val_accuracy: 0.2375\n",
      "Epoch 436/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.4373 - accuracy: 0.8606 - val_loss: 6.3373 - val_accuracy: 0.2375\n",
      "Epoch 437/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.4451 - accuracy: 0.8588 - val_loss: 6.4108 - val_accuracy: 0.2354\n",
      "Epoch 438/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.4479 - accuracy: 0.8524 - val_loss: 6.2178 - val_accuracy: 0.2250\n",
      "Epoch 439/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.4401 - accuracy: 0.8553 - val_loss: 5.6557 - val_accuracy: 0.2333\n",
      "Epoch 440/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.4534 - accuracy: 0.8576 - val_loss: 6.2955 - val_accuracy: 0.2375\n",
      "Epoch 441/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.4406 - accuracy: 0.8618 - val_loss: 5.7613 - val_accuracy: 0.2313\n",
      "Epoch 442/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.4482 - accuracy: 0.8518 - val_loss: 6.2838 - val_accuracy: 0.2333\n",
      "Epoch 443/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.4333 - accuracy: 0.8618 - val_loss: 6.4684 - val_accuracy: 0.2354\n",
      "Epoch 444/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.4373 - accuracy: 0.8606 - val_loss: 5.9842 - val_accuracy: 0.2354\n",
      "Epoch 445/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.4394 - accuracy: 0.8547 - val_loss: 6.2581 - val_accuracy: 0.2313\n",
      "Epoch 446/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.4325 - accuracy: 0.8618 - val_loss: 6.1022 - val_accuracy: 0.2375\n",
      "Epoch 447/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.4284 - accuracy: 0.8647 - val_loss: 6.4448 - val_accuracy: 0.2292\n",
      "Epoch 448/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.4346 - accuracy: 0.8571 - val_loss: 5.9349 - val_accuracy: 0.2333\n",
      "Epoch 449/700\n",
      "1700/1700 [==============================] - 1s 626us/step - loss: 0.4334 - accuracy: 0.8629 - val_loss: 6.0342 - val_accuracy: 0.2313\n",
      "Epoch 450/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 0.4291 - accuracy: 0.8594 - val_loss: 5.9609 - val_accuracy: 0.2292\n",
      "Epoch 451/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.4325 - accuracy: 0.8594 - val_loss: 6.0609 - val_accuracy: 0.2333\n",
      "Epoch 452/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.4301 - accuracy: 0.8565 - val_loss: 5.9658 - val_accuracy: 0.2333\n",
      "Epoch 453/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.4207 - accuracy: 0.8571 - val_loss: 6.1621 - val_accuracy: 0.2313\n",
      "Epoch 454/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.4276 - accuracy: 0.8500 - val_loss: 6.5119 - val_accuracy: 0.2354\n",
      "Epoch 455/700\n",
      "1700/1700 [==============================] - 1s 582us/step - loss: 0.4211 - accuracy: 0.8618 - val_loss: 6.4689 - val_accuracy: 0.2354\n",
      "Epoch 456/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.4274 - accuracy: 0.8647 - val_loss: 6.4737 - val_accuracy: 0.2354\n",
      "Epoch 457/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.4258 - accuracy: 0.8629 - val_loss: 6.2814 - val_accuracy: 0.2354\n",
      "Epoch 458/700\n",
      "1700/1700 [==============================] - 1s 576us/step - loss: 0.4176 - accuracy: 0.8676 - val_loss: 6.5240 - val_accuracy: 0.2333\n",
      "Epoch 459/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.4167 - accuracy: 0.8600 - val_loss: 6.3336 - val_accuracy: 0.2354\n",
      "Epoch 460/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.4207 - accuracy: 0.8629 - val_loss: 6.2576 - val_accuracy: 0.2333\n",
      "Epoch 461/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.4105 - accuracy: 0.8724 - val_loss: 6.0240 - val_accuracy: 0.2375\n",
      "Epoch 462/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.4064 - accuracy: 0.8776 - val_loss: 6.2316 - val_accuracy: 0.2313\n",
      "Epoch 463/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.4216 - accuracy: 0.8612 - val_loss: 5.7365 - val_accuracy: 0.2313\n",
      "Epoch 464/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.4198 - accuracy: 0.8635 - val_loss: 6.1025 - val_accuracy: 0.2333\n",
      "Epoch 465/700\n",
      "1700/1700 [==============================] - 1s 620us/step - loss: 0.4175 - accuracy: 0.8582 - val_loss: 6.2718 - val_accuracy: 0.2375\n",
      "Epoch 466/700\n",
      "1700/1700 [==============================] - 1s 606us/step - loss: 0.4151 - accuracy: 0.8688 - val_loss: 6.4170 - val_accuracy: 0.2375\n",
      "Epoch 467/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.4036 - accuracy: 0.8765 - val_loss: 6.6402 - val_accuracy: 0.2396\n",
      "Epoch 468/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.4166 - accuracy: 0.8594 - val_loss: 6.2807 - val_accuracy: 0.2354\n",
      "Epoch 469/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.4033 - accuracy: 0.8647 - val_loss: 6.3237 - val_accuracy: 0.2354\n",
      "Epoch 470/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.4164 - accuracy: 0.8565 - val_loss: 6.4743 - val_accuracy: 0.2333\n",
      "Epoch 471/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.4071 - accuracy: 0.8706 - val_loss: 6.5101 - val_accuracy: 0.2354\n",
      "Epoch 472/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.4049 - accuracy: 0.8700 - val_loss: 6.4174 - val_accuracy: 0.2354\n",
      "Epoch 473/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.4110 - accuracy: 0.8741 - val_loss: 6.4799 - val_accuracy: 0.2396\n",
      "Epoch 474/700\n",
      "1700/1700 [==============================] - 1s 612us/step - loss: 0.4031 - accuracy: 0.8682 - val_loss: 6.3973 - val_accuracy: 0.2250\n",
      "Epoch 475/700\n",
      "1700/1700 [==============================] - 1s 610us/step - loss: 0.4021 - accuracy: 0.8653 - val_loss: 6.6092 - val_accuracy: 0.2354\n",
      "Epoch 476/700\n",
      "1700/1700 [==============================] - 1s 636us/step - loss: 0.4083 - accuracy: 0.8618 - val_loss: 6.4585 - val_accuracy: 0.2354\n",
      "Epoch 477/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.4051 - accuracy: 0.8724 - val_loss: 6.4132 - val_accuracy: 0.2375\n",
      "Epoch 478/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.4060 - accuracy: 0.8712 - val_loss: 6.4632 - val_accuracy: 0.2354\n",
      "Epoch 479/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.3974 - accuracy: 0.8747 - val_loss: 6.6356 - val_accuracy: 0.2354\n",
      "Epoch 480/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.3975 - accuracy: 0.8724 - val_loss: 7.0862 - val_accuracy: 0.2354\n",
      "Epoch 481/700\n",
      "1700/1700 [==============================] - 1s 644us/step - loss: 0.4003 - accuracy: 0.8653 - val_loss: 6.6490 - val_accuracy: 0.2375\n",
      "Epoch 482/700\n",
      "1700/1700 [==============================] - 1s 609us/step - loss: 0.4149 - accuracy: 0.8541 - val_loss: 6.8997 - val_accuracy: 0.2333\n",
      "Epoch 483/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.3981 - accuracy: 0.8741 - val_loss: 6.7588 - val_accuracy: 0.2313\n",
      "Epoch 484/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.4020 - accuracy: 0.8688 - val_loss: 6.8021 - val_accuracy: 0.2396\n",
      "Epoch 485/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3930 - accuracy: 0.8759 - val_loss: 6.9820 - val_accuracy: 0.2354\n",
      "Epoch 486/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.4008 - accuracy: 0.8653 - val_loss: 6.5983 - val_accuracy: 0.2354\n",
      "Epoch 487/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.3919 - accuracy: 0.8665 - val_loss: 6.7741 - val_accuracy: 0.2354\n",
      "Epoch 488/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.3918 - accuracy: 0.8724 - val_loss: 6.8726 - val_accuracy: 0.2354\n",
      "Epoch 489/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.3841 - accuracy: 0.8735 - val_loss: 6.6642 - val_accuracy: 0.2354\n",
      "Epoch 490/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3858 - accuracy: 0.8747 - val_loss: 6.7374 - val_accuracy: 0.2354\n",
      "Epoch 491/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3887 - accuracy: 0.8776 - val_loss: 6.6774 - val_accuracy: 0.2375\n",
      "Epoch 492/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.3895 - accuracy: 0.8700 - val_loss: 6.9006 - val_accuracy: 0.2354\n",
      "Epoch 493/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.3882 - accuracy: 0.8724 - val_loss: 6.5711 - val_accuracy: 0.2333\n",
      "Epoch 494/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.3891 - accuracy: 0.8724 - val_loss: 7.0401 - val_accuracy: 0.2375\n",
      "Epoch 495/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.3868 - accuracy: 0.8812 - val_loss: 6.8950 - val_accuracy: 0.2354\n",
      "Epoch 496/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3818 - accuracy: 0.8700 - val_loss: 7.0624 - val_accuracy: 0.2354\n",
      "Epoch 497/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.3796 - accuracy: 0.8724 - val_loss: 6.6938 - val_accuracy: 0.2354\n",
      "Epoch 498/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.3709 - accuracy: 0.8818 - val_loss: 6.6844 - val_accuracy: 0.2354\n",
      "Epoch 499/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.3834 - accuracy: 0.8788 - val_loss: 6.2474 - val_accuracy: 0.2313\n",
      "Epoch 500/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.3893 - accuracy: 0.8712 - val_loss: 6.8931 - val_accuracy: 0.2354\n",
      "Epoch 501/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3732 - accuracy: 0.8800 - val_loss: 6.6898 - val_accuracy: 0.2396\n",
      "Epoch 502/700\n",
      "1700/1700 [==============================] - 1s 575us/step - loss: 0.3819 - accuracy: 0.8776 - val_loss: 6.8757 - val_accuracy: 0.2354\n",
      "Epoch 503/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.3729 - accuracy: 0.8759 - val_loss: 6.6471 - val_accuracy: 0.2333\n",
      "Epoch 504/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.3693 - accuracy: 0.8835 - val_loss: 6.9871 - val_accuracy: 0.2354\n",
      "Epoch 505/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.3554 - accuracy: 0.8947 - val_loss: 6.9129 - val_accuracy: 0.2396\n",
      "Epoch 506/700\n",
      "1700/1700 [==============================] - 1s 577us/step - loss: 0.3723 - accuracy: 0.8788 - val_loss: 6.8273 - val_accuracy: 0.2417\n",
      "Epoch 507/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.3704 - accuracy: 0.8776 - val_loss: 7.4505 - val_accuracy: 0.2375\n",
      "Epoch 508/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.3733 - accuracy: 0.8818 - val_loss: 7.2772 - val_accuracy: 0.2354\n",
      "Epoch 509/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.3660 - accuracy: 0.8776 - val_loss: 6.8117 - val_accuracy: 0.2333\n",
      "Epoch 510/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3718 - accuracy: 0.8782 - val_loss: 6.7746 - val_accuracy: 0.2375\n",
      "Epoch 511/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3644 - accuracy: 0.8859 - val_loss: 6.4106 - val_accuracy: 0.2333\n",
      "Epoch 512/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.3687 - accuracy: 0.8876 - val_loss: 6.9484 - val_accuracy: 0.2375\n",
      "Epoch 513/700\n",
      "1700/1700 [==============================] - 1s 629us/step - loss: 0.3702 - accuracy: 0.8829 - val_loss: 6.6893 - val_accuracy: 0.2354\n",
      "Epoch 514/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.3731 - accuracy: 0.8771 - val_loss: 7.0327 - val_accuracy: 0.2396\n",
      "Epoch 515/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3624 - accuracy: 0.8876 - val_loss: 7.0368 - val_accuracy: 0.2375\n",
      "Epoch 516/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.3545 - accuracy: 0.8924 - val_loss: 6.8595 - val_accuracy: 0.2375\n",
      "Epoch 517/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.3547 - accuracy: 0.8859 - val_loss: 6.6417 - val_accuracy: 0.2375\n",
      "Epoch 518/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.3690 - accuracy: 0.8829 - val_loss: 6.7744 - val_accuracy: 0.2333\n",
      "Epoch 519/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.3601 - accuracy: 0.8794 - val_loss: 7.0898 - val_accuracy: 0.2396\n",
      "Epoch 520/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.3623 - accuracy: 0.8829 - val_loss: 7.1672 - val_accuracy: 0.2354\n",
      "Epoch 521/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.3525 - accuracy: 0.8947 - val_loss: 6.8214 - val_accuracy: 0.2375\n",
      "Epoch 522/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.3626 - accuracy: 0.8794 - val_loss: 6.9996 - val_accuracy: 0.2375\n",
      "Epoch 523/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.3560 - accuracy: 0.8824 - val_loss: 7.5964 - val_accuracy: 0.2396\n",
      "Epoch 524/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.3523 - accuracy: 0.8876 - val_loss: 7.2600 - val_accuracy: 0.2375\n",
      "Epoch 525/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.3411 - accuracy: 0.8971 - val_loss: 7.1183 - val_accuracy: 0.2354\n",
      "Epoch 526/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.3469 - accuracy: 0.8876 - val_loss: 7.4211 - val_accuracy: 0.2375\n",
      "Epoch 527/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.3509 - accuracy: 0.8882 - val_loss: 7.0911 - val_accuracy: 0.2354\n",
      "Epoch 528/700\n",
      "1700/1700 [==============================] - 1s 615us/step - loss: 0.3567 - accuracy: 0.8853 - val_loss: 7.3526 - val_accuracy: 0.2354\n",
      "Epoch 529/700\n",
      "1700/1700 [==============================] - 1s 665us/step - loss: 0.3456 - accuracy: 0.8929 - val_loss: 7.4472 - val_accuracy: 0.2375\n",
      "Epoch 530/700\n",
      "1700/1700 [==============================] - 1s 612us/step - loss: 0.3437 - accuracy: 0.8982 - val_loss: 7.1038 - val_accuracy: 0.2375\n",
      "Epoch 531/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3383 - accuracy: 0.8994 - val_loss: 7.2144 - val_accuracy: 0.2375\n",
      "Epoch 532/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.3559 - accuracy: 0.8882 - val_loss: 7.3250 - val_accuracy: 0.2354\n",
      "Epoch 533/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3370 - accuracy: 0.8965 - val_loss: 7.2492 - val_accuracy: 0.2375\n",
      "Epoch 534/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.3523 - accuracy: 0.8871 - val_loss: 7.1034 - val_accuracy: 0.2375\n",
      "Epoch 535/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3435 - accuracy: 0.8841 - val_loss: 6.6362 - val_accuracy: 0.2313\n",
      "Epoch 536/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.3404 - accuracy: 0.8947 - val_loss: 7.1674 - val_accuracy: 0.2396\n",
      "Epoch 537/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.3459 - accuracy: 0.8924 - val_loss: 7.3684 - val_accuracy: 0.2375\n",
      "Epoch 538/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3443 - accuracy: 0.8888 - val_loss: 7.0060 - val_accuracy: 0.2375\n",
      "Epoch 539/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.3332 - accuracy: 0.8988 - val_loss: 7.0507 - val_accuracy: 0.2375\n",
      "Epoch 540/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.3408 - accuracy: 0.9006 - val_loss: 7.6124 - val_accuracy: 0.2375\n",
      "Epoch 541/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3403 - accuracy: 0.8929 - val_loss: 7.0528 - val_accuracy: 0.2354\n",
      "Epoch 542/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3367 - accuracy: 0.8924 - val_loss: 7.3116 - val_accuracy: 0.2354\n",
      "Epoch 543/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.3395 - accuracy: 0.8894 - val_loss: 6.9344 - val_accuracy: 0.2354\n",
      "Epoch 544/700\n",
      "1700/1700 [==============================] - 1s 614us/step - loss: 0.3274 - accuracy: 0.8953 - val_loss: 6.9135 - val_accuracy: 0.2292\n",
      "Epoch 545/700\n",
      "1700/1700 [==============================] - 1s 623us/step - loss: 0.3294 - accuracy: 0.8953 - val_loss: 7.1322 - val_accuracy: 0.2354\n",
      "Epoch 546/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3423 - accuracy: 0.8894 - val_loss: 7.1852 - val_accuracy: 0.2375\n",
      "Epoch 547/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3268 - accuracy: 0.8971 - val_loss: 6.9106 - val_accuracy: 0.2354\n",
      "Epoch 548/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3369 - accuracy: 0.8929 - val_loss: 7.2861 - val_accuracy: 0.2354\n",
      "Epoch 549/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3280 - accuracy: 0.8924 - val_loss: 7.1062 - val_accuracy: 0.2396\n",
      "Epoch 550/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.3141 - accuracy: 0.9047 - val_loss: 7.3435 - val_accuracy: 0.2333\n",
      "Epoch 551/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3331 - accuracy: 0.8959 - val_loss: 7.6438 - val_accuracy: 0.2396\n",
      "Epoch 552/700\n",
      "1700/1700 [==============================] - 1s 608us/step - loss: 0.3394 - accuracy: 0.8953 - val_loss: 7.0084 - val_accuracy: 0.2396\n",
      "Epoch 553/700\n",
      "1700/1700 [==============================] - 1s 601us/step - loss: 0.3389 - accuracy: 0.8906 - val_loss: 7.2293 - val_accuracy: 0.2375\n",
      "Epoch 554/700\n",
      "1700/1700 [==============================] - 1s 607us/step - loss: 0.3243 - accuracy: 0.8900 - val_loss: 7.5450 - val_accuracy: 0.2354\n",
      "Epoch 555/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 0.3257 - accuracy: 0.9035 - val_loss: 7.1437 - val_accuracy: 0.2396\n",
      "Epoch 556/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.3411 - accuracy: 0.8882 - val_loss: 7.3178 - val_accuracy: 0.2292\n",
      "Epoch 557/700\n",
      "1700/1700 [==============================] - 1s 603us/step - loss: 0.3210 - accuracy: 0.9053 - val_loss: 7.8704 - val_accuracy: 0.2396\n",
      "Epoch 558/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.3344 - accuracy: 0.8953 - val_loss: 7.3130 - val_accuracy: 0.2375\n",
      "Epoch 559/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.3234 - accuracy: 0.9024 - val_loss: 7.1879 - val_accuracy: 0.2250\n",
      "Epoch 560/700\n",
      "1700/1700 [==============================] - 1s 633us/step - loss: 0.3222 - accuracy: 0.8953 - val_loss: 7.2660 - val_accuracy: 0.2375\n",
      "Epoch 561/700\n",
      "1700/1700 [==============================] - 1s 616us/step - loss: 0.3288 - accuracy: 0.8976 - val_loss: 7.4378 - val_accuracy: 0.2354\n",
      "Epoch 562/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.3166 - accuracy: 0.9094 - val_loss: 7.2832 - val_accuracy: 0.2354\n",
      "Epoch 563/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3173 - accuracy: 0.9041 - val_loss: 7.0039 - val_accuracy: 0.2417\n",
      "Epoch 564/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3142 - accuracy: 0.9024 - val_loss: 7.4241 - val_accuracy: 0.2271\n",
      "Epoch 565/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3122 - accuracy: 0.9065 - val_loss: 7.8047 - val_accuracy: 0.2354\n",
      "Epoch 566/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3145 - accuracy: 0.9012 - val_loss: 7.5370 - val_accuracy: 0.2375\n",
      "Epoch 567/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.3107 - accuracy: 0.8959 - val_loss: 8.0498 - val_accuracy: 0.2417\n",
      "Epoch 568/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.3235 - accuracy: 0.8959 - val_loss: 7.8138 - val_accuracy: 0.2396\n",
      "Epoch 569/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3132 - accuracy: 0.9088 - val_loss: 8.0294 - val_accuracy: 0.2375\n",
      "Epoch 570/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.3209 - accuracy: 0.8971 - val_loss: 7.5143 - val_accuracy: 0.2354\n",
      "Epoch 571/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.3068 - accuracy: 0.9059 - val_loss: 7.6206 - val_accuracy: 0.2375\n",
      "Epoch 572/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.3105 - accuracy: 0.9029 - val_loss: 7.7668 - val_accuracy: 0.2375\n",
      "Epoch 573/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.3102 - accuracy: 0.9047 - val_loss: 8.0965 - val_accuracy: 0.2375\n",
      "Epoch 574/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.3008 - accuracy: 0.9053 - val_loss: 7.2084 - val_accuracy: 0.2396\n",
      "Epoch 575/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.3177 - accuracy: 0.9041 - val_loss: 8.0853 - val_accuracy: 0.2375\n",
      "Epoch 576/700\n",
      "1700/1700 [==============================] - 1s 631us/step - loss: 0.3042 - accuracy: 0.9000 - val_loss: 7.6705 - val_accuracy: 0.2375\n",
      "Epoch 577/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.3125 - accuracy: 0.9029 - val_loss: 7.8234 - val_accuracy: 0.2375\n",
      "Epoch 578/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.3062 - accuracy: 0.9000 - val_loss: 7.8326 - val_accuracy: 0.2396\n",
      "Epoch 579/700\n",
      "1700/1700 [==============================] - 1s 586us/step - loss: 0.3100 - accuracy: 0.8971 - val_loss: 7.6541 - val_accuracy: 0.2396\n",
      "Epoch 580/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.3023 - accuracy: 0.9118 - val_loss: 7.5997 - val_accuracy: 0.2396\n",
      "Epoch 581/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.3093 - accuracy: 0.9047 - val_loss: 7.5281 - val_accuracy: 0.2333\n",
      "Epoch 582/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3014 - accuracy: 0.9094 - val_loss: 8.2819 - val_accuracy: 0.2354\n",
      "Epoch 583/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.3038 - accuracy: 0.9059 - val_loss: 7.8226 - val_accuracy: 0.2375\n",
      "Epoch 584/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3034 - accuracy: 0.9082 - val_loss: 8.1312 - val_accuracy: 0.2375\n",
      "Epoch 585/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.2992 - accuracy: 0.9088 - val_loss: 8.0890 - val_accuracy: 0.2396\n",
      "Epoch 586/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.2984 - accuracy: 0.9076 - val_loss: 8.1765 - val_accuracy: 0.2375\n",
      "Epoch 587/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.3044 - accuracy: 0.9100 - val_loss: 7.4244 - val_accuracy: 0.2354\n",
      "Epoch 588/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.3038 - accuracy: 0.9006 - val_loss: 7.6443 - val_accuracy: 0.2354\n",
      "Epoch 589/700\n",
      "1700/1700 [==============================] - 1s 589us/step - loss: 0.2952 - accuracy: 0.9100 - val_loss: 8.0347 - val_accuracy: 0.2313\n",
      "Epoch 590/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.2875 - accuracy: 0.9082 - val_loss: 7.8489 - val_accuracy: 0.2396\n",
      "Epoch 591/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.2864 - accuracy: 0.9100 - val_loss: 7.7705 - val_accuracy: 0.2375\n",
      "Epoch 592/700\n",
      "1700/1700 [==============================] - 1s 631us/step - loss: 0.2904 - accuracy: 0.9094 - val_loss: 7.9778 - val_accuracy: 0.2375\n",
      "Epoch 593/700\n",
      "1700/1700 [==============================] - 1s 610us/step - loss: 0.2837 - accuracy: 0.9206 - val_loss: 7.9432 - val_accuracy: 0.2417\n",
      "Epoch 594/700\n",
      "1700/1700 [==============================] - 1s 610us/step - loss: 0.2900 - accuracy: 0.9112 - val_loss: 7.7775 - val_accuracy: 0.2375\n",
      "Epoch 595/700\n",
      "1700/1700 [==============================] - 1s 609us/step - loss: 0.2864 - accuracy: 0.9147 - val_loss: 8.7179 - val_accuracy: 0.2417\n",
      "Epoch 596/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.2941 - accuracy: 0.9053 - val_loss: 7.7254 - val_accuracy: 0.2333\n",
      "Epoch 597/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.2872 - accuracy: 0.9129 - val_loss: 7.8793 - val_accuracy: 0.2354\n",
      "Epoch 598/700\n",
      "1700/1700 [==============================] - 1s 587us/step - loss: 0.2892 - accuracy: 0.9141 - val_loss: 7.9147 - val_accuracy: 0.2375\n",
      "Epoch 599/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.2913 - accuracy: 0.9071 - val_loss: 7.8491 - val_accuracy: 0.2354\n",
      "Epoch 600/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.2900 - accuracy: 0.9135 - val_loss: 7.6289 - val_accuracy: 0.2354\n",
      "Epoch 601/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.2856 - accuracy: 0.9147 - val_loss: 7.8086 - val_accuracy: 0.2333\n",
      "Epoch 602/700\n",
      "1700/1700 [==============================] - 1s 594us/step - loss: 0.2926 - accuracy: 0.9088 - val_loss: 7.9889 - val_accuracy: 0.2375\n",
      "Epoch 603/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.2910 - accuracy: 0.9041 - val_loss: 8.3495 - val_accuracy: 0.2354\n",
      "Epoch 604/700\n",
      "1700/1700 [==============================] - 1s 592us/step - loss: 0.2890 - accuracy: 0.9088 - val_loss: 8.0340 - val_accuracy: 0.2417\n",
      "Epoch 605/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.2783 - accuracy: 0.9176 - val_loss: 7.8903 - val_accuracy: 0.2354\n",
      "Epoch 606/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.2735 - accuracy: 0.9171 - val_loss: 8.2566 - val_accuracy: 0.2417\n",
      "Epoch 607/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.2857 - accuracy: 0.9112 - val_loss: 8.0546 - val_accuracy: 0.2375\n",
      "Epoch 608/700\n",
      "1700/1700 [==============================] - 1s 624us/step - loss: 0.2822 - accuracy: 0.9153 - val_loss: 8.4877 - val_accuracy: 0.2375\n",
      "Epoch 609/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.2886 - accuracy: 0.9071 - val_loss: 8.1743 - val_accuracy: 0.2375\n",
      "Epoch 610/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.2714 - accuracy: 0.9241 - val_loss: 8.3949 - val_accuracy: 0.2375\n",
      "Epoch 611/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.2779 - accuracy: 0.9182 - val_loss: 8.0556 - val_accuracy: 0.2375\n",
      "Epoch 612/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.2687 - accuracy: 0.9188 - val_loss: 8.1340 - val_accuracy: 0.2354\n",
      "Epoch 613/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.2773 - accuracy: 0.9153 - val_loss: 8.5309 - val_accuracy: 0.2375\n",
      "Epoch 614/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.2641 - accuracy: 0.9241 - val_loss: 8.2165 - val_accuracy: 0.2333\n",
      "Epoch 615/700\n",
      "1700/1700 [==============================] - 1s 579us/step - loss: 0.2776 - accuracy: 0.9147 - val_loss: 8.6072 - val_accuracy: 0.2396\n",
      "Epoch 616/700\n",
      "1700/1700 [==============================] - 1s 577us/step - loss: 0.2708 - accuracy: 0.9147 - val_loss: 8.3149 - val_accuracy: 0.2375\n",
      "Epoch 617/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.2666 - accuracy: 0.9200 - val_loss: 7.9058 - val_accuracy: 0.2333\n",
      "Epoch 618/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.2711 - accuracy: 0.9194 - val_loss: 7.9446 - val_accuracy: 0.2354\n",
      "Epoch 619/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.2631 - accuracy: 0.9188 - val_loss: 7.8403 - val_accuracy: 0.2375\n",
      "Epoch 620/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.2665 - accuracy: 0.9212 - val_loss: 8.2564 - val_accuracy: 0.2375\n",
      "Epoch 621/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.2730 - accuracy: 0.9106 - val_loss: 8.2284 - val_accuracy: 0.2375\n",
      "Epoch 622/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.2626 - accuracy: 0.9253 - val_loss: 8.2021 - val_accuracy: 0.2333\n",
      "Epoch 623/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.2702 - accuracy: 0.9182 - val_loss: 8.0283 - val_accuracy: 0.2396\n",
      "Epoch 624/700\n",
      "1700/1700 [==============================] - 1s 630us/step - loss: 0.2620 - accuracy: 0.9218 - val_loss: 8.3734 - val_accuracy: 0.2396\n",
      "Epoch 625/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.2723 - accuracy: 0.9094 - val_loss: 8.0335 - val_accuracy: 0.2354\n",
      "Epoch 626/700\n",
      "1700/1700 [==============================] - 1s 580us/step - loss: 0.2654 - accuracy: 0.9176 - val_loss: 8.6045 - val_accuracy: 0.2396\n",
      "Epoch 627/700\n",
      "1700/1700 [==============================] - 1s 607us/step - loss: 0.2673 - accuracy: 0.9253 - val_loss: 8.9552 - val_accuracy: 0.2375\n",
      "Epoch 628/700\n",
      "1700/1700 [==============================] - 1s 625us/step - loss: 0.2625 - accuracy: 0.9206 - val_loss: 7.9767 - val_accuracy: 0.2354\n",
      "Epoch 629/700\n",
      "1700/1700 [==============================] - 1s 603us/step - loss: 0.2619 - accuracy: 0.9271 - val_loss: 8.3698 - val_accuracy: 0.2375\n",
      "Epoch 630/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.2621 - accuracy: 0.9206 - val_loss: 8.4047 - val_accuracy: 0.2354\n",
      "Epoch 631/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.2588 - accuracy: 0.9206 - val_loss: 8.5681 - val_accuracy: 0.2375\n",
      "Epoch 632/700\n",
      "1700/1700 [==============================] - 1s 600us/step - loss: 0.2581 - accuracy: 0.9247 - val_loss: 8.5842 - val_accuracy: 0.2417\n",
      "Epoch 633/700\n",
      "1700/1700 [==============================] - 1s 595us/step - loss: 0.2625 - accuracy: 0.9194 - val_loss: 8.7005 - val_accuracy: 0.2375\n",
      "Epoch 634/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.2518 - accuracy: 0.9282 - val_loss: 7.6483 - val_accuracy: 0.2375\n",
      "Epoch 635/700\n",
      "1700/1700 [==============================] - 1s 591us/step - loss: 0.2716 - accuracy: 0.9129 - val_loss: 8.8037 - val_accuracy: 0.2375\n",
      "Epoch 636/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.2565 - accuracy: 0.9194 - val_loss: 7.9936 - val_accuracy: 0.2375\n",
      "Epoch 637/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.2638 - accuracy: 0.9200 - val_loss: 8.5167 - val_accuracy: 0.2396\n",
      "Epoch 638/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.2556 - accuracy: 0.9194 - val_loss: 8.7997 - val_accuracy: 0.2375\n",
      "Epoch 639/700\n",
      "1700/1700 [==============================] - 1s 617us/step - loss: 0.2593 - accuracy: 0.9194 - val_loss: 8.2542 - val_accuracy: 0.2375\n",
      "Epoch 640/700\n",
      "1700/1700 [==============================] - 1s 620us/step - loss: 0.2476 - accuracy: 0.9312 - val_loss: 9.0102 - val_accuracy: 0.2417\n",
      "Epoch 641/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.2471 - accuracy: 0.9259 - val_loss: 8.4636 - val_accuracy: 0.2354\n",
      "Epoch 642/700\n",
      "1700/1700 [==============================] - 1s 590us/step - loss: 0.2507 - accuracy: 0.9188 - val_loss: 8.3985 - val_accuracy: 0.2396\n",
      "Epoch 643/700\n",
      "1700/1700 [==============================] - 1s 584us/step - loss: 0.2587 - accuracy: 0.9265 - val_loss: 8.6653 - val_accuracy: 0.2417\n",
      "Epoch 644/700\n",
      "1700/1700 [==============================] - 1s 583us/step - loss: 0.2521 - accuracy: 0.9259 - val_loss: 8.1483 - val_accuracy: 0.2354\n",
      "Epoch 645/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.2582 - accuracy: 0.9224 - val_loss: 8.0499 - val_accuracy: 0.2354\n",
      "Epoch 646/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.2447 - accuracy: 0.9282 - val_loss: 8.4970 - val_accuracy: 0.2396\n",
      "Epoch 647/700\n",
      "1700/1700 [==============================] - 1s 599us/step - loss: 0.2480 - accuracy: 0.9282 - val_loss: 8.1886 - val_accuracy: 0.2396\n",
      "Epoch 648/700\n",
      "1700/1700 [==============================] - 1s 643us/step - loss: 0.2470 - accuracy: 0.9235 - val_loss: 8.6044 - val_accuracy: 0.2375\n",
      "Epoch 649/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.2438 - accuracy: 0.9324 - val_loss: 8.2289 - val_accuracy: 0.2354\n",
      "Epoch 650/700\n",
      "1700/1700 [==============================] - 1s 593us/step - loss: 0.2437 - accuracy: 0.9271 - val_loss: 8.7993 - val_accuracy: 0.2354\n",
      "Epoch 651/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.2546 - accuracy: 0.9188 - val_loss: 8.9784 - val_accuracy: 0.2417\n",
      "Epoch 652/700\n",
      "1700/1700 [==============================] - 1s 598us/step - loss: 0.2466 - accuracy: 0.9241 - val_loss: 8.3046 - val_accuracy: 0.2375\n",
      "Epoch 653/700\n",
      "1700/1700 [==============================] - 1s 597us/step - loss: 0.2441 - accuracy: 0.9253 - val_loss: 8.7009 - val_accuracy: 0.2375\n",
      "Epoch 654/700\n",
      "1700/1700 [==============================] - 1s 596us/step - loss: 0.2539 - accuracy: 0.9171 - val_loss: 8.8767 - val_accuracy: 0.2375\n",
      "Epoch 655/700\n",
      "1700/1700 [==============================] - 1s 632us/step - loss: 0.2420 - accuracy: 0.9318 - val_loss: 8.6533 - val_accuracy: 0.2354\n",
      "Epoch 656/700\n",
      "1700/1700 [==============================] - 1s 604us/step - loss: 0.2428 - accuracy: 0.9276 - val_loss: 9.0639 - val_accuracy: 0.2375\n",
      "Epoch 657/700\n",
      "1700/1700 [==============================] - 1s 581us/step - loss: 0.2385 - accuracy: 0.9288 - val_loss: 9.0173 - val_accuracy: 0.2375\n",
      "Epoch 658/700\n",
      "1700/1700 [==============================] - 1s 588us/step - loss: 0.2490 - accuracy: 0.9276 - val_loss: 8.4972 - val_accuracy: 0.2333\n",
      "Epoch 659/700\n",
      "1700/1700 [==============================] - 1s 585us/step - loss: 0.2406 - accuracy: 0.9271 - val_loss: 8.6017 - val_accuracy: 0.2354\n",
      "Epoch 660/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700/1700 [==============================] - 1s 571us/step - loss: 0.2335 - accuracy: 0.9259 - val_loss: 8.8215 - val_accuracy: 0.2354\n",
      "Epoch 661/700\n",
      "1700/1700 [==============================] - 1s 571us/step - loss: 0.2386 - accuracy: 0.9406 - val_loss: 8.8580 - val_accuracy: 0.2396\n",
      "Epoch 662/700\n",
      "1700/1700 [==============================] - 1s 574us/step - loss: 0.2297 - accuracy: 0.9324 - val_loss: 8.9540 - val_accuracy: 0.2375\n",
      "Epoch 663/700\n",
      "1700/1700 [==============================] - 1s 570us/step - loss: 0.2453 - accuracy: 0.9253 - val_loss: 8.3806 - val_accuracy: 0.2375\n",
      "Epoch 664/700\n",
      "1700/1700 [==============================] - 1s 574us/step - loss: 0.2348 - accuracy: 0.9253 - val_loss: 8.6040 - val_accuracy: 0.2396\n",
      "Epoch 665/700\n",
      "1700/1700 [==============================] - 1s 576us/step - loss: 0.2350 - accuracy: 0.9265 - val_loss: 8.4638 - val_accuracy: 0.2292\n",
      "Epoch 666/700\n",
      "1700/1700 [==============================] - 1s 577us/step - loss: 0.2346 - accuracy: 0.9306 - val_loss: 8.9531 - val_accuracy: 0.2375\n",
      "Epoch 667/700\n",
      "1700/1700 [==============================] - 1s 572us/step - loss: 0.2332 - accuracy: 0.9294 - val_loss: 8.9394 - val_accuracy: 0.2375\n",
      "Epoch 668/700\n",
      "1700/1700 [==============================] - 1s 573us/step - loss: 0.2414 - accuracy: 0.9353 - val_loss: 8.8151 - val_accuracy: 0.2396\n",
      "Epoch 669/700\n",
      "1700/1700 [==============================] - 1s 578us/step - loss: 0.2324 - accuracy: 0.9365 - val_loss: 8.8031 - val_accuracy: 0.2333\n",
      "Epoch 670/700\n",
      "1264/1700 [=====================>........] - ETA: 0s - loss: 0.2365 - accuracy: 0.9272"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700,validation_data=(x_testcnn, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['accuracy'])\n",
    "plt.plot(cnnhistory.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 40, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 40, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 7)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
